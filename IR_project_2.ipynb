{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "549d14426afb2109edb71ef6e0223d5b",
     "grade": false,
     "grade_id": "cell-133a4667b3e842fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 2: Learning to Rank <a class=\"anchor\" id=\"toptop\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "188d0bd6218df31e0a8795322a1b9912",
     "grade": false,
     "grade_id": "cell-9409dd22f820096c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Submission instructions**:\n",
    "- Only the code `TODO: Implement this!` denotes that these sections are graded. Please do not delete the comment #YOUR CODE HERE.\n",
    "- Vectoriize your code instead of using for-loop when implementing Neural Nets.\n",
    "- The notebook you submit has to have the student ids, separated by underscores (E.g., `12341234_12341234_12341234.ipynb`). \n",
    "- This will be parsed by a regexp, so please double check your filename.\n",
    "- Only one member of each group has to submit the file to canvas.\n",
    "- Make sure to check that your notebook runs before submission. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "- Please do not delete/add new cells. Removing cells can lead to grade deduction. Also do not change the number of parameters in the pre-defined functions.\n",
    "- Note, that you are not allowed to use Google Colab.\n",
    "\n",
    "**Learning Goals**:\n",
    "- Part 1: Offline LTR\n",
    "  - Learn how to implement pointwise, pairwise and listwise algorithms for learning to rank \n",
    "- Part 2: Online LTR\n",
    "  - Implement learning to rank algorithms from historical clicks and online evaluation of ranking algorithms.\n",
    "- Learn their weaknesses & strengths and when each method is suitable. \n",
    "\n",
    "\n",
    "\n",
    "**Files to submit along with the completed notebook**:\n",
    "- `pointwise_regression.json`\n",
    "- `pointwise_classification.json`\n",
    "- `pairwise.json`\n",
    "- `listwise.json`\n",
    "- `biased_model.json'\n",
    "- `unbiased_model.json'\n",
    "\n",
    "\n",
    "---\n",
    "**Recommended Reading**:\n",
    "- Part 1:\n",
    "  - Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. InProceedings of the 22nd international conference on Machine learning, pages 89–96, 2005.\n",
    "  - Christopher J Burges, Robert Ragno, and Quoc V Le. Learning to rank with nonsmooth cost functions. In Advances inneural information processing systems, pages 193–200, 2007\n",
    "  - (Sections 1, 2 and 4) Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81, 2010\n",
    "  \n",
    "\n",
    "Additional Resources: \n",
    "- This assignment requires knowledge of [PyTorch](https://pytorch.org/). If you are unfamiliar with PyTorch, you can go over [these series of tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "\n",
    "In the previous assignment, you experimented with retrieval with different ranking functions and in addition, different document representations. \n",
    "\n",
    "This assignment deals directly with learning to rank (LTR). In offline LTR (Part 1), You will learn how to implement methods from the three approaches associated with learning to rank: pointwise, pairwise and listwise. \n",
    "\n",
    "In Part 2, you will learn about online LTR. Instead of using manually judged datasets, in online LTR, we learn from user interactions. You will learn how to simulate clicks using click models, how to learn unbiasedly from historical clicks and how to evaluate different rankers in an online environment using multileaving methods. \n",
    "\n",
    "**Note:**\n",
    "  - The dataset used in this assignment is +100Mb in size. You may need around 2Gb of RAM for running the whole notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a595c150f39970fb6ad72e463fe8b44",
     "grade": false,
     "grade_id": "cell-09127508ac207429",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "[Back to top](#toptop)\n",
    "\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "\n",
    " - [Chapter 1: Offline LTR](#o_LTR) (345 points)\n",
    "     - [Section 1: Dataset and Utility](#dataU) \n",
    "     - [Section 2: Pointwtise LTR](#pointwiseLTR) (55 points)\n",
    "     - [Section 3: Pairwise LTR](#pairwiseLTR) (60 points)\n",
    "     - [Section 4: Pairwise Speed-up RankNet](#SpairwiseLTR) (70 points)\n",
    "     - [Section 5: Listwise LTR](#listwiseLTR) (80 points)\n",
    "     - [Section 6: Evaluation](#evaluation1) (70 points)\n",
    " - [Chapter 2: Online LTR](#onLTR) (180 points)\n",
    "     - [Section 1: Clicks Simulation](#clicks) (15 points)\n",
    "     - [Section 2: Counterfactual](#cLTR) (90 points)\n",
    "     - [Section 3: Online Evaluation](#on_eval) (75 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7be29958190a403c77402e97c21c5252",
     "grade": false,
     "grade_id": "cell-b08a635cb01047dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import dataset\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f126271fe03e0c82c752179a1293748",
     "grade": false,
     "grade_id": "cell-ef602d983baa9d90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Chapter 1: Offline LTR <a class=\"anchor\" id=\"o_LTR\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83d5c5098ff7e903a1d4475f78d028be",
     "grade": false,
     "grade_id": "cell-9978e0796016b961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A typical setup of learning to rank involves a feature vector constructed using a query-document pair, and a set of relevance judgements. You are given a set of triples (`query`, `document`, `relevance grade`); where relevance grade is an *ordinal* variable  with  5  grades,  for example: {`perfect`,`excellent`,`good`,`fair`,`bad`),  typically  labeled  by human annotators.  \n",
    "\n",
    "In this assignment, you are already given the feature vector for a given document and query pair. To access these vectors, see the following code cells (note: the dataset will be automatically downloaded & the first time the next cell runs, it will take a while!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62aa687b659ad249d6b6190d4b1f7d9e",
     "grade": false,
     "grade_id": "cell-d60b3e2cd8d41210",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1: Data and Utility <a class=\"anchor\" id=\"dataU\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "First let's get familiar with the dataset and some utility methods for our implementations.\n",
    "\n",
    "### Section 1.1 Dataset stats\n",
    "\n",
    "| Split Name | \\# queries | \\# docs | \\# features |\n",
    "| :- | :--: | :--: | :--: |\n",
    "| train | 2735 | 85227 | 501 |\n",
    "| validation | 403 | 12794 | 501 |\n",
    "| test | 949 | 29881 | 501 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e11c95b755f0b252276313365c6ff290",
     "grade": false,
     "grade_id": "cell-d4779843ecb42649",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset.download_dataset()\n",
    "data = dataset.get_dataset()\n",
    "# there is only 1 fold for this dataset \n",
    "data = data.get_data_folds()[0]\n",
    "# read in the data\n",
    "data.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8008b140d6012489be5056ec30e90444",
     "grade": false,
     "grade_id": "cell-2a79356db5683374",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 501\n",
      "Split: train\n",
      "\tNumber of queries 2735\n",
      "\tNumber of docs 85227\n",
      "Split: validation\n",
      "\tNumber of queries 403\n",
      "\tNumber of docs 12794\n",
      "Split: test\n",
      "\tNumber of queries 949\n",
      "\tNumber of docs 29881\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features: {data.num_features}\")\n",
    "# print some statistics\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"Split: {split}\")\n",
    "    split = getattr(data, split)\n",
    "    print(f\"\\tNumber of queries {split.num_queries()}\")\n",
    "    print(f\"\\tNumber of docs {split.num_docs()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70b764af87765e64827eb896b0ad8643",
     "grade": false,
     "grade_id": "cell-5b034476f52f28bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 1.2 Utility classes/methods\n",
    "\n",
    "The following cells contain code that will be useful for the assigment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb52800727e7a5fe81c92706c34e6471",
     "grade": false,
     "grade_id": "cell-4ad2f0d8e4f66d37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# these is a useful class to create torch DataLoaders, and can be used during training\n",
    "class LTRData(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert split is not None, \"Invalid split!\"\n",
    "        features, labels = split.feature_matrix, split.label_vector\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i], self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61170cd9d5a02b3f9e23364bf7d46c95",
     "grade": false,
     "grade_id": "cell-6be5d30fd0264dc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 501]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## example \n",
    "train_dl = DataLoader(LTRData(data, \"train\"), batch_size=32, shuffle=True)\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets \n",
    "# - (of course, without the break statement!)\n",
    "for (x, y) in train_dl:\n",
    "    print(x.size(), y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50bdb8c74b13357983e5f5f435b70115",
     "grade": false,
     "grade_id": "cell-a79c0f58db4af010",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`evaluate_model` evaluates a model, on a given split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ca1e81dd1f55111cda0a04093fd223b",
     "grade": false,
     "grade_id": "cell-b66759e20b89e0b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this function evaluates a model, on a given split\n",
    "def evaluate_model(pred_fn, split, batch_size=256, print_results=False, q_level=False):\n",
    "    dl = DataLoader(LTRData(data, split), batch_size=batch_size)\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    for (x, y) in tqdm(dl, desc=f'Eval ({split})', leave=False):\n",
    "        all_labels.append(y.squeeze().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = pred_fn(x)\n",
    "            all_scores.append(output.squeeze().numpy())\n",
    "            \n",
    "    split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "    }.get(split)   \n",
    "    results = evaluate.evaluate2(np.asarray(all_scores), np.asarray(all_labels), print_results=print_results, q_level=q_level)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c605f95e2cd732774f1813a69bb8c3fc",
     "grade": false,
     "grade_id": "cell-66bc9b1a832d14d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 88.5522 (19.46644)\n",
      "dcg@03: 5.8530 (3.66776)\n",
      "dcg@05: 8.1507 (4.15145)\n",
      "dcg@10: 12.8411 (5.21220)\n",
      "dcg@20: 19.9498 (6.13971)\n",
      "ndcg: 0.7155 (0.05076)\n",
      "ndcg@03: 0.2141 (0.14207)\n",
      "ndcg@05: 0.2262 (0.12166)\n",
      "ndcg@10: 0.2587 (0.10988)\n",
      "ndcg@20: 0.3039 (0.09650)\n",
      "precision@01: 0.1800 (0.38419)\n",
      "precision@03: 0.1533 (0.20232)\n",
      "precision@05: 0.1520 (0.15263)\n",
      "precision@10: 0.1600 (0.13115)\n",
      "precision@20: 0.1610 (0.09502)\n",
      "recall@01: 0.0092 (0.02312)\n",
      "recall@03: 0.0232 (0.03381)\n",
      "recall@05: 0.0363 (0.04011)\n",
      "recall@10: 0.0769 (0.06761)\n",
      "recall@20: 0.1455 (0.09385)\n",
      "relevant rank: 96.4217 (67.09412)\n",
      "relevant rank per query: 2277.4800 (1288.80216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manolis/miniconda3/envs/ir1/lib/python3.9/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "## example \n",
    "# function that scores a given feature vector e.g a network\n",
    "net = nn.Linear(501, 1)\n",
    "# the evaluate method accepts a function. more specifically, a callable (such as pytorch modules) \n",
    "def notwork(x):\n",
    "    return net(x)\n",
    "# evaluate the function\n",
    "_ = evaluate_model(notwork, \"validation\", print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f71c11c5be87af7e7109a463a1e24c6c",
     "grade": false,
     "grade_id": "cell-66ae15ed8cb736b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell is used to generate reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d81a93ddde3c0ae3be42eba5a6ba025d",
     "grade": false,
     "grade_id": "cell-df3d4a5ebf6dece6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use to get reproducible results\n",
    "def seed(random_seed):\n",
    "    import random\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acd2f7fa9d402a7704d9f7f5fc1c2c89",
     "grade": false,
     "grade_id": "cell-a29483034efce729",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Pointwise LTR (55 points) <a class=\"anchor\" id=\"pointwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be an input feature vector, containing features for a query-document pair. Let $f: \\mathbb{R}^d \\rightarrow \\mathbb{R} $ be a function that maps this feature vector to a number $f(x)$ - either a relevance score (regression) or label (classification). The data $\\{x \\}$ are treated as feature vectors and the relevance judgements are treated as the target which we want to predict. \n",
    "\n",
    "In this section, you will implement a simple Pointwise model using either a regression or classification loss, and use the train set to train this model to predict (or classify) the relevance score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0f5f848b2a3509141e384bd4d101923",
     "grade": false,
     "grade_id": "cell-fdcb0b1bd78f6eda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.1: Neural Model (5 points)\n",
    "\n",
    "In the following cell, you will implement a simple pointwise LTR model: \n",
    "- Use a neural network to learn a Pointwise model using both a regression and a classification loss, using the relevance grades as the label. Use the following parameters: \n",
    "  - Layers: $501 (input) \\rightarrow 256 \\rightarrow o$ where $o$ is either 5 for classification or 1 for regression, where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLU`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)\n",
    "  - Note: Do not use a `nn.Softmax` function here - it will be taken care of later!\n",
    "  - This network will also be used by other methods i.e Pairwise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbaeb826028de6d6d7429ee18c90f455",
     "grade": false,
     "grade_id": "cell-e6ebad1d98f78bf0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "class NeuralModule(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes the Pointwise neural network. \n",
    "        Input: output_dim: The dimension of the output layer. In this assignment, \n",
    "                it is either 1 (regression) or 5 (classification)\n",
    "        \"\"\"\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        super(NeuralModule, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "          nn.Linear(501, 256),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(256, self.output_dim),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes in an input feature vector (of size 501) and produces the (regression/classification) output \n",
    "        Input: x: a [N, 501] tensor\n",
    "        Output: a [N, output_dim] tensor\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eebff5ba2f470a05674e79514a6ba7bc",
     "grade": false,
     "grade_id": "cell-2326178594a8f44c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "point_nn_clf = NeuralModule(5)\n",
    "point_nn_reg = NeuralModule(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "958fde08e4a9f04e633dc82bc85082dd",
     "grade": true,
     "grade_id": "cell-917f63ec6b575f59",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bba77f886ccd469eb1b3c4370c830f",
     "grade": true,
     "grade_id": "cell-bd3bbcd6d22aa9b2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test the forward function\n",
    "n = 10\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_clf(inp)\n",
    "### BEGIN HIDDEN TEST\n",
    "n = 20\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_clf(inp)\n",
    "assert out.size(0) == n\n",
    "assert out.size(1) == 5\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dfa481e9b325b14fdb26e39618d6169",
     "grade": true,
     "grade_id": "cell-1d92c755e64de89f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test the forward function\n",
    "n = 10\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_reg(inp)\n",
    "### BEGIN HIDDEN TEST\n",
    "n = 20\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_reg(inp)\n",
    "assert out.size(0) == n\n",
    "assert out.size(1) == 1\n",
    "### END HIDDEN TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Loss Functions (5 points)\n",
    "Pointwise LTR algorithms use pointwise loss functions.\n",
    "Usually, the popular loss functions for pointwise LTR are:\n",
    " - Cross entropy loss for classification (3 points)\n",
    " - Regression loss (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation (3 points):**\n",
    "Implement cross entropy loss and and then cross entropy prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a82ac14d42f8800a3fcf1fe5153dd1d3",
     "grade": false,
     "grade_id": "cell-d095f3c75bd11bc3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (2 points)\n",
    "def clf_loss(output, target):\n",
    "    \"\"\"\n",
    "    Cross entropy loss - returns a single number. \n",
    "    output: (float) tensor, shape - [N, 5] \n",
    "    target: (float/long) tensor, shape - [N]. \n",
    "    \n",
    "    Hint: This function should also handle cases when target is either long/float types \n",
    "    \"\"\"\n",
    "    assert output.size(0) == target.size(0)\n",
    "    assert output.size(1) == 5\n",
    "    # YOUR CODE HERE\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec3127e3b21ab74c2771110450b46559",
     "grade": true,
     "grade_id": "cell-eb43efcf784d82d9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your results: [1.5910069942474365, 1.600582480430603, 1.6723783016204834, 1.8065980672836304, 1.6385562419891357]\n",
      "expected results: [1.5910069942474365, 1.600582480430603, 1.6723783016204834, 1.8065979480743408, 1.6385562419891357]\n"
     ]
    }
   ],
   "source": [
    "## Test clf_loss\n",
    "g = torch.manual_seed(42)\n",
    "tests = [torch.rand(5, 5, generator=g) for _ in range(5)]\n",
    "target = torch.LongTensor([1, 2, 3, 4, 0])\n",
    "\n",
    "results = [1.5910069942474365, \n",
    "           1.600582480430603, \n",
    "           1.6723783016204834, \n",
    "           1.8065979480743408, \n",
    "           1.6385562419891357]\n",
    "\n",
    "l1 = [clf_loss(output, target).item() for output in tests]\n",
    "print(f'your results: {l1}')\n",
    "print(f'expected results: {results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1e1871d067d534d41585fe9d657244a",
     "grade": false,
     "grade_id": "cell-d01649f26022bf4c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "3# TODO: Implement this! (1 points)\n",
    "def clf_pred(inp, net):\n",
    "    \"\"\"\n",
    "    The output of the classifier network produces a [Nx5] output corresponding to \n",
    "    the relevance labels (each row does *not* add to 1!)\n",
    "    This function should predict the most probable relevance from the relevance labels\n",
    "    \n",
    "    inp: The input [N, num_features]\n",
    "    net: the neural network, takes in [N, num_features] and outputs [N, 5]\n",
    "    \n",
    "    return: a [N, 1] (long) tensor, the relevance labels  \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    outputs = net(inp)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e01b7aa6f5cebb78e604f9fa5d8da29",
     "grade": true,
     "grade_id": "cell-1f5c809567bf7f02",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your results:[3 3 2 1 2 4 4 1 3 1]\n",
      "expected:[3 3 2 1 2 4 4 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "g = torch.manual_seed(42)\n",
    "def clf_(inp):\n",
    "    return torch.rand(inp.size(0), 5, generator=g)\n",
    "\n",
    "inp = torch.rand(10, 5, generator=g)\n",
    "r = np.array([3, 3, 2, 1, 2, 4, 4, 1, 3, 1])\n",
    "p = clf_pred(inp, clf_).numpy()\n",
    "print(f'your results:{p}')\n",
    "print(f'expected:{r}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cf6191b552e4daea3f0fd4a91e15fd4",
     "grade": false,
     "grade_id": "cell-a4f04b744ef63756",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE:\n",
    "# to construct a predictor for a particular network, see this example\n",
    "# (this will be required in the next cell)\n",
    "clf_pred_fn = partial(clf_pred, net=point_nn_clf)\n",
    "# the 'net' argument doesn't need to be provided anymore!\n",
    "clf_pred_fn(torch.rand(5, data.num_features)).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e14973fb674e1e9adb553605e4e7b333",
     "grade": false,
     "grade_id": "cell-d683efd6ca306e81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (2 points):**\n",
    "Implement regression loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f2b905ddb927ed981bb294825674993",
     "grade": false,
     "grade_id": "cell-c024ed97d7100038",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (2 points)\n",
    "def reg_loss(output, target):\n",
    "    \"\"\"\n",
    "    Regression loss - returns a single number. \n",
    "    Make sure to use the TODO loss!\n",
    "    output: (float) tensor, shape - [N, 1] \n",
    "    target: (float) tensor, shape - [N]. \n",
    "    \"\"\"\n",
    "    assert target.dim() == 1\n",
    "    assert output.size(0) == target.size(0)\n",
    "    assert output.size(1) == 1\n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    criterion =nn.MSELoss ()\n",
    "    return criterion (output.squeeze(), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "064dde728a201a97f97ca266c8641a0e",
     "grade": true,
     "grade_id": "cell-24edd9d567aac9da",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your results:[4.800000190734863, 3.0, 7.599999904632568, 5.400000095367432, 0.6000000238418579]\n",
      "expected:[4.800000190734863, 3.0, 7.599999904632568, 5.400000095367432, 0.6000000238418579]\n"
     ]
    }
   ],
   "source": [
    "## Test reg_loss\n",
    "g = torch.manual_seed(42)\n",
    "output = [torch.randint(low=0, high=5, size=(5, 1), generator=g).float() for _ in range(5)]\n",
    "target = torch.randint(low=0, high=5, size=(5,), generator=g).float()\n",
    "\n",
    "l = [reg_loss(o, target).item() for o in output]\n",
    "r = [4.800000190734863, \n",
    "     3.0, \n",
    "     7.599999904632568, \n",
    "     5.400000095367432, \n",
    "     0.6000000238418579]\n",
    "print(f'your results:{l}')\n",
    "print(f'expected:{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0eddb61dde344891ea7efc8fdd67752f",
     "grade": false,
     "grade_id": "cell-0977a61ec0cfa7ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (45 points):**\n",
    "Now implement a wrapper for training a pointwise LTR, that takes the model and the loss function as input and trains the model.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 5 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Appropriate loss function & prediction function: 5 points\n",
    " - Both classification / regression models handled appropriately: 5 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dbfd2a686ecc7ab254a4e5e9b332119",
     "grade": false,
     "grade_id": "cell-9361533c572e304b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (45 points)\n",
    "def train_pointwise(net, loss, params):\n",
    "    \"\"\"\n",
    "    This function should train a Pointwise network, \n",
    "    trained based on the loss (either \"clf\" / \"reg\"). \n",
    "    \n",
    "    The network is trained using the Adam optimizer\n",
    "        \n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    \n",
    "    \n",
    "    Hints:\n",
    "    1. Use the LTRData class defined above\n",
    "    2. You will have to construct a partial function if loss=\"clf\" \n",
    "       before using it in evaluate_model() (see cells after the defn of clf_pred)\n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    loss: one of \"clf\" or \"reg\"\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert loss in {\"clf\", \"reg\"}\n",
    "    \n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = params.lr)\n",
    "    train_dl = DataLoader(LTRData(data, \"train\"), batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    for epoch in range(params.epochs): \n",
    "        running_loss = 0.0\n",
    "        for (inputs, labels) in train_dl:  # inputs: 256*501  labels:256*1\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)       \n",
    "            if loss == \"clf\":\n",
    "                labels = labels.long() #convert float tensor to long tensor\n",
    "                myloss = clf_loss(outputs, labels)\n",
    "            elif loss == \"reg\":\n",
    "                myloss = reg_loss (outputs, labels)\n",
    "            myloss.backward()\n",
    "            optimizer.step()\n",
    "        train_metrics_epoch.append (evaluate_model(notwork, \"train\", print_results=False))\n",
    "        val_metrics_epoch.append (evaluate_model(notwork, \"validation\", print_results=False))\n",
    "            \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manolis/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370175239/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change this to test your code!\n",
    "pointwise_test_params = Namespace(epochs=1, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                    metrics={\"ndcg\"})\n",
    "# uncomment to test your code\n",
    "## train a regression model\n",
    "met_reg = train_pointwise(point_nn_reg, \"reg\", pointwise_test_params)\n",
    "## train a classification model\n",
    "met_clf = train_pointwise (point_nn_clf, \"clf\", pointwise_test_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "738eaae8abdea0f21d248a0d677bc424",
     "grade": false,
     "grade_id": "cell-27ec0e0dd8a5d98d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell is used to generate reproducible results which should be submitted with the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a485ad0b3f14de6ee384fa6b42ab36f",
     "grade": false,
     "grade_id": "cell-11e8cbc591a51256",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def create_results(net, train_fn, prediction_fn, results_file, *train_params):\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    metrics = train_fn(net, *train_params)\n",
    "    net.eval()\n",
    "    test_metrics, test_qq = evaluate_model(prediction_fn, \"test\", print_results=True, q_level=True)\n",
    "    \n",
    "    \n",
    "    test_q = {}\n",
    "    for m in {\"ndcg\", \"precision@05\", \"recall@05\"}:\n",
    "        test_q[m] = test_qq[m]\n",
    "    \n",
    "    with open(results_file, \"w\") as writer:\n",
    "        json.dump({\n",
    "            \"metrics\": metrics,\n",
    "            \"test_metrics\": test_metrics,\n",
    "            \"test_query_level_metrics\": test_q,\n",
    "        }, writer, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5926b542770cebafc36574bbff9b7d3e",
     "grade": false,
     "grade_id": "cell-16ed543545863f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now use the above functions to generate your `json` files for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eef82389f05e60a59c9bddefc6570264",
     "grade": false,
     "grade_id": "cell-cb8314e4e579adac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 113.0552 (25.03405)\n",
      "dcg@03: 21.6713 (8.09174)\n",
      "dcg@05: 27.8372 (10.23033)\n",
      "dcg@10: 36.4737 (12.81272)\n",
      "dcg@20: 46.7620 (14.73025)\n",
      "ndcg: 0.8723 (0.05308)\n",
      "ndcg@03: 0.7423 (0.22804)\n",
      "ndcg@05: 0.7211 (0.19760)\n",
      "ndcg@10: 0.6923 (0.16486)\n",
      "ndcg@20: 0.6789 (0.13736)\n",
      "precision@01: 0.8462 (0.36080)\n",
      "precision@03: 0.7578 (0.29437)\n",
      "precision@05: 0.6974 (0.27626)\n",
      "precision@10: 0.5641 (0.23514)\n",
      "precision@20: 0.4372 (0.19206)\n",
      "recall@01: 0.0389 (0.02715)\n",
      "recall@03: 0.0992 (0.05347)\n",
      "recall@05: 0.1538 (0.08467)\n",
      "recall@10: 0.2390 (0.11141)\n",
      "recall@20: 0.3594 (0.13448)\n",
      "relevant rank: 63.4852 (62.37967)\n",
      "relevant rank per query: 1633.7949 (980.89681)\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "params_regr = Namespace(epochs=11, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "\n",
    "pointwise_regression_model = NeuralModule(1)\n",
    "create_results(pointwise_regression_model, \n",
    "               train_pointwise, \n",
    "               pointwise_regression_model,\n",
    "               \"./pointwise_regression.json\", \n",
    "               \"reg\", params_regr)\n",
    "# persist models\n",
    "torch.save(pointwise_regression_model.state_dict(), \"./pointwise_regr_wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ddac4c7614df42d2655e2304eba1c21",
     "grade": false,
     "grade_id": "cell-8b25e13a53ad95ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 108.3847 (24.75186)\n",
      "dcg@03: 19.5420 (8.70680)\n",
      "dcg@05: 24.6340 (10.50565)\n",
      "dcg@10: 31.9088 (13.10119)\n",
      "dcg@20: 40.1932 (14.39773)\n",
      "ndcg: 0.8354 (0.05743)\n",
      "ndcg@03: 0.6631 (0.25442)\n",
      "ndcg@05: 0.6315 (0.21245)\n",
      "ndcg@10: 0.6002 (0.17913)\n",
      "ndcg@20: 0.5802 (0.14183)\n",
      "precision@01: 0.7521 (0.43177)\n",
      "precision@03: 0.6752 (0.31566)\n",
      "precision@05: 0.6085 (0.27598)\n",
      "precision@10: 0.4786 (0.23564)\n",
      "precision@20: 0.3530 (0.17484)\n",
      "recall@01: 0.0353 (0.02922)\n",
      "recall@03: 0.0909 (0.05883)\n",
      "recall@05: 0.1328 (0.07951)\n",
      "recall@10: 0.1987 (0.10513)\n",
      "recall@20: 0.2823 (0.11354)\n",
      "relevant rank: 75.8755 (68.66386)\n",
      "relevant rank per query: 1952.6581 (1017.40702)\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "params_clf = Namespace(epochs=13, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "\n",
    "pointwise_classification_model = NeuralModule(5)\n",
    "create_results(pointwise_classification_model,\n",
    "               train_pointwise,\n",
    "               partial(clf_pred, net=pointwise_classification_model),\n",
    "               \"./pointwise_classification.json\", \"clf\", params_clf)\n",
    "\n",
    "torch.save(pointwise_classification_model.state_dict(), \"./pointwise_clf_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ab42dfe7a4ffdce839f7f3990147830",
     "grade": true,
     "grade_id": "cell-ccb9cdbf2280bcff",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8123d91619096913bd0809c27781857",
     "grade": true,
     "grade_id": "cell-780585f47729739e",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(\"./pointwise_regression.json\")\n",
    "assert os.path.exists(\"./pointwise_classification.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8a82c097df5799c15c9000475ad11e",
     "grade": false,
     "grade_id": "cell-e48bb26c37eacea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3: Pairwise LTR (60 points) <a class=\"anchor\" id=\"pairwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this section,  you will learn and implement RankNet, a  pairwise learning to rank algorithm.\n",
    "\n",
    "For a given query, consider two documents $D_i$ and $D_j$ with two different ground truth relevance  labels,  with  feature  vectors $x_i$ and $x_j$ respectively.   The  RankNet  model,  just  like  the pointwise model, uses $f$ to predict scores i.e $s_i=f(x_i)$ and $s_j=f(x_j)$, but uses a different loss during  training. $D_i \\triangleright D_j$ denotes  the  event  that $D_i$ should  be  ranked  higher  than $D_j$.   The  two outputs $s_i$ and $s_j$ are mapped to a learned probability that $D_i \\triangleright D_j$: \n",
    "\n",
    "\n",
    "$$        P_{ij} = \\frac{1}{1 + e^{-\\sigma(s_i - s_j)}} $$\n",
    "  \n",
    "where $\\sigma$ is a parameter that determines the shape of the sigmoid. The loss of the RankNet model is the cross entropy cost function:\n",
    "\n",
    "$$        C = - \\bar{P}_{ij} \\log P_{ij} - (1-\\bar{P}_{ij}) \\log (1 - P_{ij}) $$\n",
    "\n",
    "As the name suggests, in the pairwise approach to LTR, we optimize a loss $l$ over pairs of documents. Let $S_{ij} \\in \\{0, \\pm1 \\}$ be equal to $1$ if the relevance of document $i$ is greater than document $j$; $-1$ if document $j$ is more relevant than document $i$; and 0 if they have the same relevance. This gives us $\\bar{P}_{ij} = \\frac{1}{2} (1 + S_{ij})$ so that $\\bar{P}_{ij} = 1$ if $D_i \\triangleright D_j$; $\\bar{P}_{ij} = 0$ if $D_j \\triangleright D_i$; and finally $\\bar{P}_{ij} = \\frac{1}{2}$ if the relevance is identical. This gives us:\n",
    "\n",
    "$$        C = \\frac{1}{2}(1- S_{ij})\\sigma(s_i - s_j) + \\log(1+ e^{-\\sigma(s_i - s_j)}) $$\n",
    "\n",
    "Now, consider a single query for which $n$ documents have been returned. Let the output scores of the ranker be $s_j$ ; $j=\\{1, \\dots, n \\}$, the model parameters be $w_k \\in \\mathbb{R}^W$, and let the set of pairs of document indices used for training be $\\mathcal{P}$. Then, the total cost is $C_T = \\sum_{i,j \\in \\mathcal{P}} C(s_i; s_j)$. \n",
    "\n",
    "\n",
    "\n",
    "- Implement RankNet. You should construct training samples by creating all possible pairs of documents for a given query and optimizing the loss above. Use the following parameters:\n",
    "  - Layers: $501 (input) \\rightarrow 256 \\rightarrow 1$, where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLu`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e80a1fc2830a7bfe3be62c3bbf1df5b7",
     "grade": false,
     "grade_id": "cell-5359ecd282448c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the pairwise loss, we need to have a structured **dataloader** which detects the documents associated with a specific query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e50a3b3ef0bf4fba2f792073ebb8443",
     "grade": false,
     "grade_id": "cell-0009b5254fc5f2ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1048 has 16 query-document pairs\n",
      "Shape of features for Query 1048: torch.Size([16, 501])\n"
     ]
    }
   ],
   "source": [
    "class QueryGroupedLTRData(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        self.split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert self.split is not None, \"Invalid split!\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.split.num_queries()\n",
    "\n",
    "    def __getitem__(self, q_i):\n",
    "        feature = torch.FloatTensor(self.split.query_feat(q_i))\n",
    "        labels = torch.FloatTensor(self.split.query_labels(q_i))\n",
    "        return q_i, feature, labels\n",
    "\n",
    "# the return types are different from what pytorch expects, \n",
    "# so we will define a custom collate function which takes in\n",
    "# a batch and returns tensors (qids, features, labels) \n",
    "def qg_collate_fn(batch):\n",
    "    \n",
    "    qids = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for (q, f, l) in batch:\n",
    "        qids.append(q)\n",
    "        features.append(f)\n",
    "        labels.append(l)\n",
    "    \n",
    "    return qids, features, labels\n",
    "    \n",
    "    \n",
    "## example - NOTE the collate_fn argument!\n",
    "train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), batch_size=1, shuffle=True, collate_fn=qg_collate_fn)\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets \n",
    "for (qids, x, y) in train_dl:\n",
    "    # different from the previous data loader, qids, x and y aren't tensors, but lists!\n",
    "    for q_i, features_i, labels_i in zip(qids, x, y):\n",
    "        print(f\"Query {q_i} has {len(features_i)} query-document pairs\")\n",
    "        print(f\"Shape of features for Query {q_i}: {features_i.size()}\")\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c719c1aca6aa05893f903f78d75ba94",
     "grade": false,
     "grade_id": "cell-acdb1bfcd2ec582e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (25 points):**\n",
    "First, implement the pairwaise loss, described above.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Proper application of the formula: 10 points\n",
    " - Mean loss: 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dcefb2b21b4524aa03cdf22382934ba",
     "grade": false,
     "grade_id": "cell-3a612aeb9e982639",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# # TODO: Implement this! (25 points)\n",
    "def pairwise_loss (scores, labels):\n",
    "    \"\"\"\n",
    "    Compute and return the pairwise loss *for a single query*. \n",
    "    To compute this, compute the loss for each ordering in a query, \n",
    "    and then return the mean. Use sigma=1.\n",
    "    \n",
    "    For a query, consider all possible ways of comparing 2 document-query pairs.\n",
    "    \n",
    "    Hint: See the next cell for an example which should make it clear how the inputs look like\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    \"\"\"\n",
    "    if labels.size(0) < 2:\n",
    "        return None\n",
    "    Si_Sj = torch.FloatTensor ([i[0]-i[1] for i in itertools.combinations (scores,2)])\n",
    "    label_comb =[i for i in itertools.combinations (labels,2)]\n",
    "    \n",
    "    Sij = []\n",
    "    for l in label_comb:\n",
    "        if l[0]>l[1]:\n",
    "            Sij.append (1)\n",
    "        elif l[0]<l[1]:\n",
    "            Sij.append (-1)\n",
    "        elif l[0]==l[1]:\n",
    "            Sij.append (0)\n",
    "    Sij= torch.FloatTensor(Sij)\n",
    "    sigma =1\n",
    "    loss = 0.5*(1-Sij)*sigma*(Si_Sj)+ torch.log (1+ torch.exp(-sigma*(Si_Sj)))\n",
    "    loss = torch.mean (loss)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "234a4669f7c7e14949006be676cabb90",
     "grade": false,
     "grade_id": "cell-01f6e909bc892bc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6869)\n",
      "tensor(0.2014)\n"
     ]
    }
   ],
   "source": [
    "# Let's say we have 2 queries, the first one with 5 <document, query> pairs \n",
    "#    and the second one with 2 <document, query> pairs. The two variables can\n",
    "#    look something like this (note the shape, not the values):\n",
    "\n",
    "scores_1 = torch.FloatTensor([0.2, 2.3, 4.5, 0.2, 1.0])\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7])\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "print(pairwise_loss(scores_1, labels_1))\n",
    "print(pairwise_loss(scores_2, labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "618bd72120cdd1baae3f22733f124d6c",
     "grade": true,
     "grade_id": "cell-5f706c38e99721df",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b63a41669c7a768f5420d43968a53212",
     "grade": false,
     "grade_id": "cell-45f14561e4843320",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (35 points):**\n",
    "Now implement the wrapper for the pairwise LTR.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset\n",
    " - and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31ad65b3cbd923af54ff56f804bbc93c",
     "grade": false,
     "grade_id": "cell-a85c38ca94031203",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (35 points)\n",
    "def train_pairwise(net, params):\n",
    "    \"\"\"\n",
    "    This function should train the given network using the pairwise loss\n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models\n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    Note: You can assume params.batch_size will always be equal to 1\n",
    "    \n",
    "    Hint: Consider the case when the loss function returns 'None'\n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "    \"\"\"\n",
    "\n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = params.lr)\n",
    "    train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), \n",
    "                          batch_size=params.batch_size, shuffle=True,  collate_fn=qg_collate_fn)\n",
    "    \n",
    "    for epoch in range(params.epochs): \n",
    "        for (qids, x, y) in train_dl:\n",
    "            for q_i, features_i, labels_i in zip(qids, x, y):                                          # features_i=x=N*501,  labels_i=y=N  N:total num of query-document pairs\n",
    "                \n",
    "                net.zero_grad()\n",
    "                scores = net(features_i)\n",
    "                loss = pairwise_loss(scores, labels_i)\n",
    "                if loss is None:\n",
    "                    continue\n",
    "                loss.requires_grad_(True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        train_metrics_epoch.append (evaluate_model(notwork, \"train\", print_results=False))\n",
    "        val_metrics_epoch.append (evaluate_model(notwork, \"validation\", print_results=False))\n",
    "\n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 90.4318 (20.11002)\n",
      "dcg@03: 5.6490 (4.34446)\n",
      "dcg@05: 7.9325 (5.42162)\n",
      "dcg@10: 12.0510 (6.41400)\n",
      "dcg@20: 18.5434 (7.85599)\n",
      "ndcg: 0.6988 (0.05507)\n",
      "ndcg@03: 0.2088 (0.17515)\n",
      "ndcg@05: 0.2202 (0.16094)\n",
      "ndcg@10: 0.2404 (0.13514)\n",
      "ndcg@20: 0.2770 (0.11648)\n",
      "precision@01: 0.1368 (0.34359)\n",
      "precision@03: 0.1538 (0.23686)\n",
      "precision@05: 0.1556 (0.21421)\n",
      "precision@10: 0.1504 (0.14943)\n",
      "precision@20: 0.1521 (0.11720)\n",
      "recall@01: 0.0045 (0.01238)\n",
      "recall@03: 0.0171 (0.02635)\n",
      "recall@05: 0.0290 (0.03754)\n",
      "recall@10: 0.0569 (0.05255)\n",
      "recall@20: 0.1182 (0.08192)\n",
      "relevant rank: 118.9286 (77.12491)\n",
      "relevant rank per query: 3060.6325 (1445.24377)\n"
     ]
    }
   ],
   "source": [
    "pairwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "## uncomment to test your code\n",
    "pairwise_net = NeuralModule(1)\n",
    "train_pairwise(pairwise_net, pairwise_params_test)\n",
    "pairwise_test, pairwise_q = evaluate_model(pairwise_net,\n",
    "                                         \"test\", print_results=True, q_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 89.4173 (19.40641)\n",
      "dcg@03: 4.5696 (3.47208)\n",
      "dcg@05: 6.3722 (4.15926)\n",
      "dcg@10: 10.1813 (5.04731)\n",
      "dcg@20: 16.1300 (6.55027)\n",
      "ndcg: 0.6918 (0.05211)\n",
      "ndcg@03: 0.1642 (0.12966)\n",
      "ndcg@05: 0.1740 (0.12206)\n",
      "ndcg@10: 0.2029 (0.10509)\n",
      "ndcg@20: 0.2418 (0.09677)\n",
      "precision@01: 0.0855 (0.27958)\n",
      "precision@03: 0.0798 (0.15500)\n",
      "precision@05: 0.0889 (0.14663)\n",
      "precision@10: 0.0991 (0.10978)\n",
      "precision@20: 0.1060 (0.09409)\n",
      "recall@01: 0.0030 (0.01014)\n",
      "recall@03: 0.0090 (0.01858)\n",
      "recall@05: 0.0177 (0.02935)\n",
      "recall@10: 0.0380 (0.04148)\n",
      "recall@20: 0.0794 (0.06469)\n",
      "relevant rank: 121.0608 (72.96104)\n",
      "relevant rank per query: 3115.5043 (1460.60763)\n"
     ]
    }
   ],
   "source": [
    "pairwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "## uncomment to test your code\n",
    "pairwise_net = NeuralModule(1)\n",
    "train_pairwise(pairwise_net, pairwise_params_test)\n",
    "pairwise_test, pairwise_q = evaluate_model(pairwise_net,\n",
    "                                         \"test\", print_results=True, q_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4b97202b1befddb4af2ec7851c5d174",
     "grade": false,
     "grade_id": "cell-19ec0cf692c86b75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pairwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "## uncomment to test your code\n",
    "# pairwise_net = NeuralModule(1)\n",
    "# train_pairwise(pairwise_net, pairwise_params_test)\n",
    "# pairwise_test, pairwise_q = evaluate_model(pairwise_net,\n",
    "#                                          \"test\", print_results=True, q_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52558867a0a7f79d6c42ceab4f26a132",
     "grade": true,
     "grade_id": "cell-34178113ea5e9331",
     "locked": false,
     "points": 35,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3867bfe2e108bffb3ae69f5ddfd68834",
     "grade": false,
     "grade_id": "cell-3a95bb01f72fc76c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 4: Pairwise: Speed-up RankNet (70 points) <a class=\"anchor\" id=\"SpairwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "To speed up training of the previous model, we can consider a sped up version of the model, where instead of `.backward` on the loss, we use `torch.backward(lambda_i)`. \n",
    "\n",
    "The derivative of the total cost $C_T$ with respect to the model parameters $w_k$ is:\n",
    "\n",
    "$$        \\frac{\\partial C_T}{\\partial w_k} = \\sum_{(i,j) \\in \\mathcal{P}} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial C(s_i, s_j)}{\\partial s_j} \\frac{\\partial s_j}{\\partial w_k} $$\n",
    "\n",
    "We can rewrite this sum by considering the set of indices $j$ , for which $\\{i,j\\}$ is a valid pair, denoted by $\\mathcal{P}_i$, and the set of document indices $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\sum_{j \\in \\mathcal{P}_i} \n",
    "\\frac{\\partial C(s_i, s_j)}{\\partial s_i} \n",
    "$$\n",
    "\n",
    "This sped of version of the algorithm first computes scores $s_i$ for all the documents. Then for each $j= 1, \\dots, n$, compute:\n",
    "\n",
    "$$\n",
    "\\lambda_{ij} = \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sigma \\bigg( \\frac{1}{2}(1 - S_{ij}) -  \\frac{1}{1 + e^{\\sigma(s_i -s_j))}} \\bigg) \\\\\n",
    "\\lambda_i = \\sum_{j \\in \\mathcal{P}_i} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sum_{j \\in \\mathcal{P}_i} \\lambda_{ij}\n",
    "$$\n",
    "\n",
    "That gives us:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\lambda_i\n",
    "$$\n",
    "\n",
    "This can be directly optimized in pytorch using: `torch.autograd.backward(scores, lambda_i)` \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c88f76d295dd1dc2778cd2413b4c58e8",
     "grade": false,
     "grade_id": "cell-2a9b7b682a011642",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (20 points):**\n",
    "Implement the sped-up version of pairwise loss, described above.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Proper application of the formula: 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4cea38cdd68cb70d08e33827bffc53a8",
     "grade": false,
     "grade_id": "cell-ba7f8d8631e3f1d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def compute_lambda_i(scores, labels):\n",
    "    \"\"\"\n",
    "    Compute \\lambda_i (defined in the previous cell). (assume sigma=1.)\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    return: \\lambda_i, a tensor of shape: [N, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    if labels.size(0) < 2:\n",
    "        return None\n",
    "\n",
    "    scores_transp = scores.view(len(scores), 1)\n",
    "    si_sj = scores-scores_transp\n",
    "\n",
    "    Sij = torch.empty(len (labels), len (labels))\n",
    "    for i in range (0, len (labels)):\n",
    "        for j in range (0, len(labels)):\n",
    "                if labels[i]>labels[j]:\n",
    "                    Sij[i][j]=1\n",
    "                elif labels[i]<labels[j]:\n",
    "                    Sij[i][j]=-1\n",
    "                elif labels[i]==labels[j]:\n",
    "                    Sij[i][j]=0\n",
    "    Sij= torch.FloatTensor(Sij)\n",
    "\n",
    "    sigma =1 \n",
    "    lambda_ij= sigma * (0.5*(1- Sij)- 1/(1+torch.exp( -sigma*(si_sj))) )\n",
    "    lambda_i = torch.sum(lambda_ij, dim=1)\n",
    "    lambda_i = lambda_i.view(len(lambda_i),1) \n",
    "\n",
    "    return lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c4533f39d3dfbefdd4b56c61316eac8",
     "grade": true,
     "grade_id": "cell-756179237da34c57",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00482b2615dc28ead4c745a705e5fafb",
     "grade": false,
     "grade_id": "cell-ed55c62fbba08923",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (50 points):**\n",
    "Using the sped-up loss function `compute_lambda_i`, implement the spedup wrapper for pairwise training.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14e4da9878b2e0a3603437b29a66a07c",
     "grade": false,
     "grade_id": "cell-ddfeb927c2f4a31c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (50 points)\n",
    "def train_pairwise_spedup(net, params):\n",
    "    \"\"\"\n",
    "    This function should train the given network using the sped up pairwise loss\n",
    "    \n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    Note: You can assume params.batch_size will always be equal to 1\n",
    "    \n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models\n",
    "    \"\"\"\n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = params.lr)\n",
    "    train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), \n",
    "                          batch_size=params.batch_size, shuffle=True,  collate_fn=qg_collate_fn)\n",
    "    \n",
    "    for epoch in range(params.epochs): \n",
    "        for (qids, x, y) in train_dl:\n",
    "            for q_i, features_i, labels_i in zip(qids, x, y):     # features_i=x=N*501,  labels_i=y=N  N:total num of query-document pairs\n",
    "                net.zero_grad()\n",
    "                scores = net(features_i)\n",
    "                lambda_i = compute_lambda_i (scores, labels_i)  \n",
    "                torch.autograd.backward(scores, lambda_i)  \n",
    "                optimizer.step()\n",
    "        train_metrics_epoch.append (evaluate_model(notwork, \"train\", print_results=False))\n",
    "        val_metrics_epoch.append (evaluate_model(notwork, \"validation\", print_results=False))    \n",
    "    \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'metrics_val': [{'dcg': (88.54593197584042, 19.464697900157525),\n",
       "   'dcg@03': (5.853035954357219, 3.667756188032379),\n",
       "   'dcg@05': (8.104309371512757, 4.196143402574963),\n",
       "   'dcg@10': (12.835644041656208, 5.216583261830074),\n",
       "   'dcg@20': (19.94386615206439, 6.141233400210386),\n",
       "   'ndcg': (0.7154592153142579, 0.05080912655392841),\n",
       "   'ndcg@03': (0.21407885604587867, 0.1420696337799491),\n",
       "   'ndcg@05': (0.22510252356395324, 0.12279541499458897),\n",
       "   'ndcg@10': (0.2586326916765895, 0.10997793251169083),\n",
       "   'ndcg@20': (0.30386023125028827, 0.09656028972560284),\n",
       "   'precision@01': (0.18, 0.38418745424597095),\n",
       "   'precision@03': (0.15333333333333332, 0.20231987873991356),\n",
       "   'precision@05': (0.14800000000000002, 0.15393505123915086),\n",
       "   'precision@10': (0.16, 0.13114877048604),\n",
       "   'precision@20': (0.161, 0.09502105029939419),\n",
       "   'recall@01': (0.00921846803574965, 0.023117573399105914),\n",
       "   'recall@03': (0.02320521799978156, 0.03380640912077783),\n",
       "   'recall@05': (0.0356106496251596, 0.04043459244469366),\n",
       "   'recall@10': (0.07691702775797651, 0.06760675594618645),\n",
       "   'recall@20': (0.14552598550197832, 0.0938478439579582),\n",
       "   'relevant rank': (96.42760372565623, 67.09346836419164),\n",
       "   'relevant rank per query': (2277.62, 1288.8705115720506)}],\n",
       " 'metrics_train': [{'dcg': (88.64967299475184, 20.387179232819307),\n",
       "   'dcg@03': (5.827048002724236, 4.057536379794524),\n",
       "   'dcg@05': (8.097391083822897, 4.629941797767148),\n",
       "   'dcg@10': (12.56704752390455, 5.70853373822541),\n",
       "   'dcg@20': (19.546572785655165, 6.953372495126278),\n",
       "   'ndcg': (0.7167057765016166, 0.054579400527607916),\n",
       "   'ndcg@03': (0.21521100153709627, 0.15185601690713513),\n",
       "   'ndcg@05': (0.2284035423103152, 0.13526183298324865),\n",
       "   'ndcg@10': (0.25842042799527865, 0.1213547576110748),\n",
       "   'ndcg@20': (0.30213722497856715, 0.10476467714642533),\n",
       "   'precision@01': (0.15015015015015015, 0.35721853613724686),\n",
       "   'precision@03': (0.13813813813813813, 0.20891924481592897),\n",
       "   'precision@05': (0.14414414414414414, 0.1701534467434672),\n",
       "   'precision@10': (0.14924924924924923, 0.1365854770078664),\n",
       "   'precision@20': (0.1539039039039039, 0.10576370235345531),\n",
       "   'recall@01': (0.0070517187641723385, 0.0238285833004197),\n",
       "   'recall@03': (0.01882296456451131, 0.034335695989793975),\n",
       "   'recall@05': (0.032363766152462274, 0.04379870610635045),\n",
       "   'recall@10': (0.06708317446634232, 0.0733935185193073),\n",
       "   'recall@20': (0.13632382750158142, 0.0955097297012578),\n",
       "   'relevant rank': (96.65680697762609, 67.11058425006495),\n",
       "   'relevant rank per query': (2296.252252252252, 1242.7911853416065)}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to test your code\n",
    "pairwise_spedup_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg@10\",\"ndcg\"})\n",
    "pairwise_net_spedup = NeuralModule(1)\n",
    "train_pairwise_spedup(pairwise_net_spedup, pairwise_spedup_params_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb9b27b6ec712d8d1146751bc150791e",
     "grade": false,
     "grade_id": "cell-f80dc2eae24968ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to test your code\n",
    "# pairwise_spedup_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg@10\",\"ndcg\"})\n",
    "# pairwise_net_spedup = NeuralModule(1)\n",
    "# train_pairwise_spedup(pairwise_net_spedup, pairwise_spedup_params_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c173b7020a1122f7218bbf37ee370884",
     "grade": false,
     "grade_id": "cell-20aa326063f673c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell creates the results file you will have to submit. *Note that the next cell trains and evaluates only the sped-up version - this is intentional!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77211db554a4bb37582d7c61c7170243",
     "grade": false,
     "grade_id": "cell-32660e207ce6d16a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 98.9575 (20.71072)\n",
      "dcg@03: 9.1353 (5.19445)\n",
      "dcg@05: 12.7020 (6.30224)\n",
      "dcg@10: 18.5124 (7.26078)\n",
      "dcg@20: 27.5377 (8.93087)\n",
      "ndcg: 0.7666 (0.05517)\n",
      "ndcg@03: 0.3269 (0.19194)\n",
      "ndcg@05: 0.3479 (0.18232)\n",
      "ndcg@10: 0.3684 (0.15120)\n",
      "ndcg@20: 0.4118 (0.12790)\n",
      "precision@01: 0.3248 (0.46829)\n",
      "precision@03: 0.3105 (0.30422)\n",
      "precision@05: 0.2991 (0.26490)\n",
      "precision@10: 0.2778 (0.20090)\n",
      "precision@20: 0.2538 (0.16055)\n",
      "recall@01: 0.0135 (0.02283)\n",
      "recall@03: 0.0376 (0.04044)\n",
      "recall@05: 0.0627 (0.05952)\n",
      "recall@10: 0.1159 (0.09279)\n",
      "recall@20: 0.2030 (0.11594)\n",
      "relevant rank: 80.8070 (63.34079)\n",
      "relevant rank per query: 2079.5726 (1104.16875)\n"
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "params = Namespace(epochs=8, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=1,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "pairwise_model = NeuralModule(1)\n",
    "\n",
    "create_results(pairwise_model, \n",
    "               train_pairwise_spedup, \n",
    "               pairwise_model,\n",
    "               \"./pairwise.json\",\n",
    "               params)\n",
    "# persist model\n",
    "torch.save(pairwise_model.state_dict(), \"./pairwise_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cc6f1a702be838b6e26733b089d5bda",
     "grade": true,
     "grade_id": "cell-8aad7ba1f8c6c23c",
     "locked": false,
     "points": 35,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b46d1ebc1676bc1dbce629b0a978326",
     "grade": true,
     "grade_id": "cell-3dc8ee835753e5aa",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(\"./pairwise.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60ad8126d68fcf4814988ad22c335c2a",
     "grade": false,
     "grade_id": "cell-14e048f55b2e6aea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Section 5: Listwise LTR (80 points) <a class=\"anchor\" id=\"listwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this section, you will implement LambdaRank, a listwise approach to LTR. Consider the computation of $\\lambda$ for sped-up RankNet (that you've already implemented). $\\lambda$ here amounts to the 'force' on a document given its neighbours in the ranked list. The design of $\\lambda$ in LambdaRank is similar to RankNet, but is scaled by DCG gain from swapping the two documents in question. Let's suppose that the corresponding ranks of doucment $D_i$ and $D_j$ are $r_i$ and $r_j$ respectively. Given a ranking measure $IRM$, such as $NDCG$ or $ERR$, the lambda function in LambdaRank is defined as:\n",
    "\n",
    "\n",
    "$$        \\frac{\\partial C}{\\partial s_i} = \\sum_{j \\in D} \\lambda_{ij} \\cdot |\\bigtriangleup IRM (i,j)| $$\n",
    "\n",
    "Where $|\\bigtriangleup IRM(i,j)|$ is the absolute difference in $IRM$ after swapping the rank positions $r_i$ and $r_j$ while leaving everything else unchanged ($| \\cdot |$ denotes the absolute value). Note that we do not backpropogate $|\\bigtriangleup IRM|$, it is treated as a constant that scales the gradients. In this assignment we will use $|\\bigtriangleup NDCG|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0caedecf5dcfd8b561a99f58ea756abd",
     "grade": false,
     "grade_id": "cell-351c194e6797d0a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (30 points):**\n",
    "Implement the listwise loss.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Computing $|\\bigtriangleup NDCG|$: 10 points \n",
    " - Proper application of the formula: 10 points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0ba4266e951c80236d3dc6f0e0c32a5",
     "grade": false,
     "grade_id": "cell-48f6a2a1c4a529b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (30 points)\n",
    "\n",
    "def DCG (labels, denom):                                 # label:[N] # denominator:[N] \n",
    "    return torch.sum ((2**labels-1)/denom)               # return a scalar\n",
    "\n",
    "def lambda_ij (scores, labels):\n",
    "    \n",
    "    scores_transp = scores.view(len(scores), 1)\n",
    "    si_sj = scores-scores_transp\n",
    "\n",
    "    Sij = torch.empty(len (labels), len (labels))\n",
    "    for i in range (0, len (labels)):\n",
    "        for j in range (0, len(labels)):\n",
    "                if labels[i]>labels[j]:\n",
    "                    Sij[i][j]=1\n",
    "                elif labels[i]<labels[j]:\n",
    "                    Sij[i][j]=-1\n",
    "                elif labels[i]==labels[j]:\n",
    "                    Sij[i][j]=0\n",
    "    Sij= torch.FloatTensor(Sij)\n",
    "\n",
    "    sigma =1 \n",
    "    lambda_ij= sigma * (0.5*(1- Sij)- 1/(1+torch.exp( -sigma*(si_sj))) )\n",
    "    return lambda_ij\n",
    "\n",
    "\n",
    "def listwise_loss(scores, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the LambdaRank loss. (assume sigma=1.)\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    returns: a tensor of size [N, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    denom = torch.log2 (torch.range(1, len(labels))+1)\n",
    "\n",
    "    odcg = DCG (labels, denom)       # oDCG: original labels, unsorted labels\n",
    "    ideal_labels= torch.sort(labels, descending=True).values\n",
    "    idcg= DCG (ideal_labels, denom)\n",
    "\n",
    "\n",
    "    swaped_DCG = torch.zeros(len(labels), len(labels))\n",
    "    for i in range (0, len(labels)):\n",
    "        for j in range (0, len(labels)):  \n",
    "            li=labels[i]\n",
    "            lj=labels[j]\n",
    "            swaped_labels = labels.clone().detach()\n",
    "            swaped_labels[j]=li\n",
    "            swaped_labels[i]=lj\n",
    "            swaped_DCG[i][j] = DCG (swaped_labels, denom)\n",
    "    delta_DCG = odcg - swaped_DCG\n",
    "    delta_nDCG = torch.abs (delta_DCG/idcg)\n",
    "    \n",
    "    lambda_ij_matrix =lambda_ij (scores, labels)\n",
    "    new_grad = delta_nDCG * lambda_ij_matrix\n",
    "    lambda_i = torch.sum(new_grad, dim=1)\n",
    "    lambda_i = lambda_i.view(len(lambda_i),1)\n",
    "    return lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28f708cd49db29860a2774a320685d18",
     "grade": false,
     "grade_id": "cell-7ffca014e68d2c32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1170],\n",
      "        [ 0.1034],\n",
      "        [ 0.0365],\n",
      "        [ 0.0263],\n",
      "        [-0.2832]])\n",
      "tensor([[-0.0529],\n",
      "        [ 0.0529]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-456d4387a26a>:39: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  denom = torch.log2 (torch.range(1, len(labels))+1)\n"
     ]
    }
   ],
   "source": [
    "scores_1 = torch.FloatTensor([0.2, 2.3, 4.5, 0.2, 1.0])\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7])\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "\n",
    "print(listwise_loss(scores_1, labels_1))\n",
    "\n",
    "\n",
    "\n",
    "print(listwise_loss(scores_2, labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3c154e9791120fcb899928865f40d30",
     "grade": true,
     "grade_id": "cell-fadab94bb19ea7ee",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48df2b36472c6f03f84a293fea5d3602",
     "grade": false,
     "grade_id": "cell-7dec2c279f77b272",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### **Implementation (50 points):**\n",
    "And use the loss function above to train a listwise LTR.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "786e51ea263d0ea82a5ec251344f7d0e",
     "grade": false,
     "grade_id": "cell-34c4d0bdbb753580",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (50 points)\n",
    "def train_listwise(net, params):\n",
    "    \"\"\"\n",
    "    This function should train the given network using the listwise (LambdaRank) loss\n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    Note: You can assume params.batch_size will always be equal to 1\n",
    "    \n",
    "    \n",
    "    net: the neural network to be trained\n",
    "    \n",
    "    params: params is an object which contains config used in training \n",
    "        (eg. params.epochs - the number of epochs to train). \n",
    "        For a full list of these params, see the next cell. \n",
    "        \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models\n",
    "    \"\"\"\n",
    "    \n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    # YOUR CODE HERE\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = params.lr)\n",
    "    train_dl = DataLoader(QueryGroupedLTRData(data, \"train\"), \n",
    "                      batch_size=params.batch_size, shuffle=True,  collate_fn=qg_collate_fn)\n",
    "    \n",
    "    for epoch in range(params.epochs): \n",
    "        for (qids, x, y) in train_dl:\n",
    "            for q_i, features_i, labels_i in zip(qids, x, y):                                          # features_i=x=N*501,  labels_i=y=N  N:total num of query-document pairs\n",
    "                net.zero_grad()\n",
    "                scores = net(features_i)\n",
    "                lambda_i = listwise_loss(scores, labels_i)\n",
    "                torch.autograd.backward(scores, lambda_i)  \n",
    "                optimizer.step()\n",
    "        train_metrics_epoch.append (evaluate_model(notwork, \"train\", print_results=False))\n",
    "        val_metrics_epoch.append (evaluate_model(notwork, \"validation\", print_results=False))    \n",
    "    \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-456d4387a26a>:39: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  denom = torch.log2 (torch.range(1, len(labels))+1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (test):   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 88.1085 (19.34824)\n",
      "dcg@03: 5.2716 (3.78388)\n",
      "dcg@05: 6.9147 (4.13924)\n",
      "dcg@10: 10.2158 (4.81863)\n",
      "dcg@20: 15.2789 (5.75678)\n",
      "ndcg: 0.6808 (0.04983)\n",
      "ndcg@03: 0.1871 (0.14108)\n",
      "ndcg@05: 0.1853 (0.11569)\n",
      "ndcg@10: 0.2017 (0.09737)\n",
      "ndcg@20: 0.2276 (0.08188)\n",
      "precision@01: 0.1282 (0.33432)\n",
      "precision@03: 0.1481 (0.21102)\n",
      "precision@05: 0.1265 (0.15819)\n",
      "precision@10: 0.1154 (0.10987)\n",
      "precision@20: 0.1021 (0.07746)\n",
      "recall@01: 0.0056 (0.01700)\n",
      "recall@03: 0.0204 (0.03332)\n",
      "recall@05: 0.0271 (0.03642)\n",
      "recall@10: 0.0472 (0.04966)\n",
      "recall@20: 0.0838 (0.06971)\n",
      "relevant rank: 129.9239 (73.96155)\n",
      "relevant rank per query: 3343.5983 (1512.18947)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dcg': (88.10852995872807, 19.348236265046786),\n",
       " 'dcg@03': (5.271614752747326, 3.783877180691881),\n",
       " 'dcg@05': (6.914655352740924, 4.13924232118727),\n",
       " 'dcg@10': (10.215758894250092, 4.818628085245252),\n",
       " 'dcg@20': (15.278875854408101, 5.7567802325250526),\n",
       " 'ndcg': (0.6808206769563825, 0.04983490248140969),\n",
       " 'ndcg@03': (0.18705967210441626, 0.14107579175237803),\n",
       " 'ndcg@05': (0.18528990007414076, 0.1156905882621576),\n",
       " 'ndcg@10': (0.20170987503193002, 0.09736644969393562),\n",
       " 'ndcg@20': (0.22762895557264318, 0.08187574543952969),\n",
       " 'precision@01': (0.1282051282051282, 0.33431807206167424),\n",
       " 'precision@03': (0.14814814814814814, 0.2110186232152844),\n",
       " 'precision@05': (0.1264957264957265, 0.15819125198303283),\n",
       " 'precision@10': (0.11538461538461539, 0.10986812966989),\n",
       " 'precision@20': (0.10213675213675216, 0.07745778072330493),\n",
       " 'recall@01': (0.005618165073293278, 0.016999354283787942),\n",
       " 'recall@03': (0.020384259600711538, 0.03332296169527206),\n",
       " 'recall@05': (0.027081714299199457, 0.03641997072036641),\n",
       " 'recall@10': (0.04724972527391271, 0.04965827999020643),\n",
       " 'recall@20': (0.08379128438510101, 0.06970550429192399),\n",
       " 'relevant rank': (129.9239455330455, 73.96155343212679),\n",
       " 'relevant rank per query': (3343.5982905982905, 1512.189469767499)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to test your code\n",
    "listwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "listwise_net = NeuralModule(1)\n",
    "train_listwise(listwise_net, listwise_params_test)\n",
    "evaluate_model(listwise_net, \"test\", print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "109d4be0105ee65f2e446c5d1a961eea",
     "grade": false,
     "grade_id": "cell-36910488e505a73a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to test your code\n",
    "# listwise_params_test = Namespace(epochs=1, lr=1e-3, batch_size=1, metrics={\"ndcg\"})\n",
    "# listwise_net = NeuralModule(1)\n",
    "# train_listwise(listwise_net, listwise_params_test)\n",
    "# evaluate_model(listwise_net, \"test\", print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "275a63cf01f8efa9cb9b42b45d41aa9b",
     "grade": false,
     "grade_id": "cell-27de808a4c9d6bf6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following cell saves your results in `listwise.json` file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b50d890fdac41f183467a165cc71ae6",
     "grade": false,
     "grade_id": "cell-8c3dde9e1dcde277",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-456d4387a26a>:39: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  denom = torch.log2 (torch.range(1, len(labels))+1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-50c5291a5069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlistwise_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m create_results(listwise_model, \n\u001b[0m\u001b[1;32m      9\u001b[0m                \u001b[0mtrain_listwise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                \u001b[0mlistwise_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-4590cbe3bc73>\u001b[0m in \u001b[0;36mcreate_results\u001b[0;34m(net, train_fn, prediction_fn, results_file, *train_params)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_qq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-5d3b361189f0>\u001b[0m in \u001b[0;36mtrain_listwise\u001b[0;34m(net, params)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mq_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                          \u001b[0;31m# features_i=x=N*501,  labels_i=y=N  N:total num of query-document pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mlambda_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-c071059ba7aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \"\"\"\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed(42)\n",
    "params = Namespace(epochs=8, \n",
    "                    lr=1e-4,\n",
    "                    batch_size=1,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "listwise_model = NeuralModule(1)\n",
    "\n",
    "create_results(listwise_model, \n",
    "               train_listwise, \n",
    "               listwise_model,\n",
    "               \"./listwise.json\",\n",
    "               params)\n",
    "\n",
    "# persist model\n",
    "torch.save(listwise_model.state_dict(), \"./listwise_wt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbccf7ada9159548390a5d8d28df92d1",
     "grade": true,
     "grade_id": "cell-6c37b60a428d000a",
     "locked": false,
     "points": 35,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e086691009a4dc0f8013c9f7f2d61bf",
     "grade": true,
     "grade_id": "cell-6c8d3b2e4ab668b1",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(\"./listwise.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a1aaed7333acfcbdf24046a15cc5a2b",
     "grade": false,
     "grade_id": "cell-e47b21d69c9be1e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 6: Comparing Pointwise, Pairwise and Listwise (70 points) <a class=\"anchor\" id=\"evaluation1\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In the next few cells, we will compare the methods you've implemented. Helper functions are provided for you, which you can use to make some conclusions. You can modify the code as needed (you are encouraged to do so!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(ax, rects, labels):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for l, rect in zip(labels, rects):\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(l),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                   fontsize=7)\n",
    "\n",
    "def compare_methods(labels, metrics, metrics_to_plot={\"ndcg\", \"precision@05\", \"recall@05\"}):\n",
    "    \"\"\"\n",
    "    Constructs bar plots to compare different methods. \n",
    "    \n",
    "    labels: list/tuple of length N\n",
    "    metrics: list/tuple of length N, containing dictionary containing the test set results \n",
    "    metrics_to_plot: set of metrics to plot - each metric creates a separate plot \n",
    "    \"\"\"\n",
    "    assert len(metrics) == len(labels)\n",
    "    \n",
    "    x = np.arange(len(metrics_to_plot)) \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(metrics_to_plot), sharey=False)\n",
    "    fig.set_figheight(7)\n",
    "    fig.set_figwidth(15)\n",
    "    \n",
    "    colors = cm.get_cmap(\"Set1\").colors\n",
    "\n",
    "    for metric, ax, c in zip(metrics_to_plot, axes, colors):\n",
    "        m = [_[metric][0] for _ in metrics]  \n",
    "        std = [_[metric][1] for _ in metrics]\n",
    "        x = np.arange(len(labels))\n",
    "        rects = ax.bar(x, m, label=metric, color=c)\n",
    "        \n",
    "        l = [\"{0:.4f}({1:.4f})\".format(_[metric][0], _[metric][1]) for _ in metrics]\n",
    "        autolabel(ax, rects, l)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "        ax.set_yticks(np.linspace(0, 1, num=11))\n",
    "        ax.set_ylim(ymin=min(m) - 0.05, ymax= max(m) + 0.05)\n",
    "        ax.set_title(metric)\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_distribution(labels, q_metrics, metric=\"ndcg\"):\n",
    "    \"\"\"\n",
    "    Plots the distribution of NDCG scores\n",
    "    \n",
    "    labels: list/tuple of length N\n",
    "    q_metrics: list/tuple of dictionaries with length N, containing the query level results\n",
    "    metric: the metric to plot\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(labels)\n",
    "    # nC2\n",
    "    n_plots = int((n*(n-1))/2)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=n_plots, ncols=1)\n",
    "    fig.set_figheight(8 * n_plots)\n",
    "    fig.set_figwidth(10)\n",
    "    \n",
    "    colors = cm.get_cmap(\"Set1\").colors\n",
    "    \n",
    "    for idx, (i, j) in enumerate(itertools.combinations(range(n), 2)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        im = q_metrics[i][metric]\n",
    "        jm = q_metrics[j][metric]\n",
    "        \n",
    "        ax.hist(im, bins=50, label=labels[i], color=colors[i], alpha=0.55)\n",
    "        ax.hist(jm, bins=50, label=labels[j], color=colors[j], alpha=0.55)\n",
    "        \n",
    "        ax.set_title(f\"{labels[i]} vs {labels[j]}\")\n",
    "        ax.legend()\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_xlabel(\"NDCG (binned)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10071c1830b37c08ea6accb26accb372",
     "grade": false,
     "grade_id": "cell-b8d4e6f2d2e78550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './listwise_wt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f9ddf9126d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlistwise_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlistwise_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./listwise_wt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ir1/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './listwise_wt'"
     ]
    }
   ],
   "source": [
    "# Load metrics and models\n",
    "\n",
    "pointwise_regr_model = NeuralModule(1)\n",
    "pointwise_regr_model.load_state_dict(torch.load(\"./pointwise_regr_wt\"))\n",
    "\n",
    "pointwise_clf_model = NeuralModule(5)\n",
    "pointwise_clf_model.load_state_dict(torch.load(\"./pointwise_clf_wt\"))\n",
    "pointwise_clf_pred_fn = partial(clf_pred, net=pointwise_clf_model)\n",
    "\n",
    "pairwise_model = NeuralModule(1)\n",
    "pairwise_model.load_state_dict(torch.load(\"./pairwise_wt\"))\n",
    "\n",
    "listwise_model = NeuralModule(1)\n",
    "listwise_model.load_state_dict(torch.load(\"./listwise_wt\"))\n",
    "\n",
    "\n",
    "methods = [\n",
    "    {\"results_file\": \"./pointwise_regression.json\", \"label\": \"Pointwise (R)\"},\n",
    "    {\"results_file\": \"./pointwise_classification.json\", \"label\": \"Pointwise (C)\"},\n",
    "    {\"results_file\": \"./pairwise.json\", \"label\": \"Pairwise\"},\n",
    "    {\"results_file\": \"./listwise.json\", \"label\": \"Listwise\"}\n",
    "]\n",
    "\n",
    "labels = []\n",
    "results = []\n",
    "q_results = []\n",
    "for m in methods:\n",
    "    labels.append(m[\"label\"])\n",
    "    \n",
    "    with open(m[\"results_file\"]) as reader:\n",
    "        r = json.load(reader)\n",
    "    \n",
    "    results.append(r[\"test_metrics\"])\n",
    "    q_results.append(r[\"test_query_level_metrics\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62939c8a4fe146ee5833f48a92142e10",
     "grade": false,
     "grade_id": "cell-c4ac703bc5a62d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-df0be1f4ae50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "compare_methods(labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa461417560d69ebe07df21a8ad9b48e",
     "grade": false,
     "grade_id": "cell-a1ea52fda896045e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e3eb188297bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "plot_distribution(labels, q_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6e424fb4065a175111443cfbf26c618",
     "grade": false,
     "grade_id": "cell-d6d65f1ab73cda9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next cell, report the performance metrics for the methods (25 points):\n",
    "\n",
    "\n",
    "\n",
    "|     Model    | NDCG (s.d)| Precision@5 (s.d) | Recall@5 (s.d) |\n",
    "|:------------:|:----------:|:-----------:|:--------:|\n",
    "| Pointwise(R) |            |             |          |\n",
    "| Pointwise(C) |            |             |          |\n",
    "|   Pairwise   |            |             |          |\n",
    "|   Listwise   |            |             |          |\n",
    "\n",
    "\n",
    "\n",
    "**Rubric:** Each reported <Method, Metric> carries 2 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-9caa39334dfb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-9caa39334dfb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    |     Model    | NDCG (s.d)      |  Precision@5 (s.d)  | Recall@5 (s.d) |\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "|     Model    | NDCG (s.d)      |  Precision@5 (s.d)  | Recall@5 (s.d) |\n",
    "|:------------:|:----------:     |:-----------:        |:--------:      |\n",
    "| Pointwise(R) |  0.8723(0.05308)|   0.6974(0.27626)   |0.1538 (0.08467)|          \n",
    "| Pointwise(C) |  0.8354(0.05743)|   0.6085 (0.27598)  |0.1328 (0.07951)|          \n",
    "|   Pairwise   |  0.7666(0.05517)|   0.2991 (0.26490)  |0.0627 (0.05952)|\n",
    "|   Listwise   |  0.6808(0.04983)|   0.1265 (0.15819)  |0.0271 (0.03642)|          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89bf20199df56a7b90fa88c132800f23",
     "grade": true,
     "grade_id": "cell-5e733f8b8649a66f",
     "locked": false,
     "points": 25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Generally we can say that our neural network converges when we reach the optimum update of our weights that lead to the minimum loss value. We  train our neural network and update the Weights tensor up until the point that the loss value in the training set starts to increase instead of decreasing. In our case the number of updating rounds, epochs are 8. The above approach may lead to overfitting, thus we may want to train our artificial neural network up to the point when the accuracy of validation loss increases and the loss does not increase that much. \n",
    "\n",
    "\n",
    "PointWise LTR performs better and generally gives better results across all metrics in comparison to Pairwise LTR. On the pairwise ltr method we mainly focus on optimising being close to the label of the document (relevant or not)  and not on its ranking. On the contrary, pairwise LTR model produces a relevant document score and computes the value of loss function, based on pairs of documents with difference in relevance. The main idea behind listwise ltr is that we optimise the ranking of the metrics directly, for example ndcg per say. \n",
    "\n",
    "\n",
    "Listwise LTR is the method that performs by far worse than all the others across all metrics if we take the mean value of them across all queries also. Though we can see that its s.d value is the least one(especially for precision@5 it is almost half of all the other models s.d) so we can conclude that it is the model with the least bias in it. \n",
    "Pointwise R model is the best, then comes pointwise C, pairwise and listwise at \n",
    "Mean value of Precision@5 for Pointwise R is approximately 70% for classification around 50%, for pairwise 30% and for listwise 10%.\n",
    "For ncdg Pointwise R gives 87,5% for C 80%, for pairwise 76% and for listwise around 67%.\n",
    "Recall@5 mean values are 15%, 10%, 6% and 2% respectively for Pointwise R,C pairwise and listwise LTR models. \n",
    "\n",
    "Pointwise Regression vs Pointwise Classification \n",
    "Plotting their NDCG together we can infer that pointwise R gives a better score .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c36dc224821c1146261aa2e8d06bd419",
     "grade": false,
     "grade_id": "cell-067c6d8584df601e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a conclusion in the next cell, considering (45 points):\n",
    "- rates of convergence\n",
    "- time complexity\n",
    "- performance wrt the 3 metrics\n",
    "- performance across queries\n",
    "- ... any other observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b8b4816351280cae45a1e498d1408fe",
     "grade": true,
     "grade_id": "cell-115db704e85b78c1",
     "locked": false,
     "points": 45,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b56e576a11e8312912751b345e72bfa",
     "grade": false,
     "grade_id": "cell-1693a9c859f14127",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Chapter 2: Online LTR (180 points) <a class=\"anchor\" id=\"onLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this part we want to use user interactions for training learning to rank algorithms.\n",
    "This part consists of the following sections:\n",
    " - Clicks Simulation (15 points)\n",
    " - Counterfactual Learning to Rank (80 points)\n",
    " - Online Evaluation (50 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "676f010e1d093ec248035d00c6888bf4",
     "grade": false,
     "grade_id": "cell-cb0180fd80d4f2cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 1: Clicks Simulation (15 points) <a class=\"anchor\" id=\"clicks\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In online LTR, we work with user interactions such as clicks.\n",
    "One way to test online LTR algorithms is to conduce semi-synthetic experiments.\n",
    "In semi-synthetic experiments, the feature vectors of a LTR dataset (similar to what you used in Part 1) are used.\n",
    "But instead of using the relevance labels for training the LTR algorithm, **simulated clicks** are used.\n",
    "\n",
    "In this section we want to simulate clicks based on the labels and use the generated clicks in later sections to train our Counterfactual LTR (CLTR) algorithm.\n",
    "\n",
    "First we need to have a *production ranker* that determines the order of documents for each query.\n",
    "The click probability of each document depends on its rank in the results list as well as its relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efe77df779addec15df70a2bd8a7aaee",
     "grade": false,
     "grade_id": "cell-4ae04e0717236f54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ranks = dataset.load_production_ranker()\n",
    "print(f'ranks has {len(ranks.keys())} keys: {ranks.keys()}')\n",
    "for key in ranks:\n",
    "    print(f'{key} shape:{ranks[key].shape}')\n",
    "    \n",
    "data.train.ranks = ranks['train']\n",
    "data.validation.ranks = ranks['valid']\n",
    "data.test.ranks = ranks['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc0ae9b0654edea0e837893b613af5b8",
     "grade": false,
     "grade_id": "cell-86dfb91f27aeb1cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (5 points):**\n",
    "Now use the `ranks` to generate clicks.\n",
    "We assume documents with label $[3,4]$ are relevant and labels $[0,1,2]$ are non-relevant.\n",
    "We also assume the examination probability of a document at rank $r$ is $\\frac{1}{r}$.\n",
    "Finally, we assume a $0.05$ noise.\n",
    "\n",
    "This means that the click probability of a document with label $[3,4]$ at rank $r$ is $\\frac{1}{r}$, while the click probability of a document with label $[0,1,2]$ at rank $r$ is $\\frac{0.05}{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bdfcf8895e430580912b057991ac7c0",
     "grade": false,
     "grade_id": "cell-6302b18566bcece2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def generate_clicks(ranking, query_labels):\n",
    "    \"\"\"\n",
    "    Generates random clicks based on the given list of ranking and relevance labels for the documents of one query.\n",
    "    Input:\n",
    "        ranking: contains the rank of documents, eg. ranking[5]=0 means that document 5 is shown at rank 0.\n",
    "        query_labels: contains the labels of documents, eg. query_labels[5]=3 means that document 5 has a label equal to 3.\n",
    "    Output:\n",
    "        A np.array of clicked positions, eq. clicked=[1,4] meanse that documents at ranks 1 and 4 have been clicked. Note that document at rank 1 may differ from document 1 (see description of ranking input).\n",
    "        \n",
    "    Hint: Use the np.random.binomial function for generating click from probability.\n",
    "    \"\"\"\n",
    "    theta = 1. / np.arange(1, ranking.shape[0] + 1)\n",
    "    noise_prob = 0.05 * theta\n",
    "    \n",
    "    zipped_ranking = list(zip(ranking,query_labels))\n",
    "    zipped_ranking = sorted(zipped_ranking, key=lambda tup: tup[0])\n",
    "    click_prob = []\n",
    "    \n",
    "    #print(zipped_ranking)\n",
    "    \n",
    "    for i,q in enumerate(zipped_ranking):\n",
    "        if q[1] in [3,4]: \n",
    "            click_prob.append((theta[i],q[0]))\n",
    "        else:\n",
    "            click_prob.append((noise_prob[i],q[0]))\n",
    "            \n",
    "    clicked = []\n",
    "            \n",
    "    for i in click_prob:\n",
    "        s = np.random.binomial(1,i[0],1)\n",
    "        if s[0] !=0 :\n",
    "            clicked.append(i[1])\n",
    "        \n",
    "           \n",
    "    clicked = np.array(clicked)    \n",
    "   \n",
    "    #raise NotImplementedError()\n",
    "    return clicked.astype(np.int32)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d5eeabc53fd3a13bb33d46f631ab3d5",
     "grade": true,
     "grade_id": "cell-098685a98a1887f4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ranking = np.array([2, 1, 3, 0, 4, 7, 5, 6])\n",
    "labels = np.array([0, 3, 4, 1, 1, 2, 4, 3])\n",
    "\n",
    "np.random.seed(4)\n",
    "print(f'clicks: {generate_clicks(ranking, labels)}')\n",
    "print('expected clicks for seed 4: [0 1 6]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fdf6ed2c6545213fb432e06dafb088e",
     "grade": false,
     "grade_id": "cell-a45b9902b9ff34c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (10 points):**\n",
    "Use `generate_clicks` to simulate clicks for different query sessions.\n",
    "For this, randomly select a query id, generate random clicks on documents of that query using your `generate_clicks` implementation and only keep the clicks on `topk` positions (i.e. do not consider clicks on ranks after `topk` rank).\n",
    "Repeat this process until you have `click_count` number of clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea8a1c5964a999322c4b30bdeecf88e6",
     "grade": false,
     "grade_id": "cell-cef4496878e0428a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def simulate_clicks(data, split_name, click_count, topk=20):\n",
    "    \"\"\"\n",
    "    Simulate click_count clicks on topk results on the specified split of the data.\n",
    "    Output:\n",
    "        A list L=[L_0, L_1, ..., L_n] of lists. Each L_i is itself a list of clicks over the documents of query number i, eg. L_0=[0,2,0,1,0,5,2] means that in the sessions of the first query, the documents at positions 0,1,2 and 5 were clicked 3,1,2 and 1 times, respectively.\n",
    "    \"\"\"\n",
    "    data_split = getattr(data, split_name)\n",
    "#     argsorted, doclist_ranges = dataset.clean_and_sort(ranks[split_name], data_split, topk)\n",
    "\n",
    "#     click_data_split = Namespace(feature_matrix = data_split.feature_matrix[argsorted,:],\n",
    "#                                  doclist_ranges = doclist_ranges,\n",
    "#                                  clicks = None)\n",
    "#     labels = data_split.label_vector[argsorted]\n",
    "\n",
    "    #num_queries = click_data_split.doclist_ranges.shape[0] - 1\n",
    "    \n",
    "    num_queries = data_split.doclist_ranges.shape[0] - 1\n",
    "    clicks = [[] for _ in range(num_queries)]\n",
    "    np.random.seed(42)\n",
    "       \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    for i,c in enumerate(clicks):\n",
    "        rng = data_split.doclist_ranges[i:i+2]\n",
    "        c.extend([gl for gl in generate_clicks(ranks[split_name][rng[0]:rng[1]], data_split.query_labels(i)).tolist() if gl<=topk])\n",
    "             \n",
    "    return clicks    \n",
    "   \n",
    "    #raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "257a37637e4baec5b5da06cfb28f9c10",
     "grade": false,
     "grade_id": "cell-d31c106145d57d5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use `simulate_clicks` function to simulate clicks for train and validation sets.\n",
    "\n",
    "**Note:**\n",
    "You can dump your generated clicks using pickle or numby and load them in your future runs to avoid long waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe427c2102d2999e8bd2a6243c0d18b6",
     "grade": true,
     "grade_id": "cell-c7adb3055f3a3d5c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_clicks = simulate_clicks(data,'train', click_count = 10000)\n",
    "\n",
    "assert isinstance(train_clicks, list)\n",
    "session_clicks = np.array(list(map(len, train_clicks)))\n",
    "print(f'{session_clicks.sum()} clicks simulated over {session_clicks.shape[0]} queries.')\n",
    "print(f'On average, each session has {session_clicks.mean()} clicks.')\n",
    "print(f'min number of clicks per session:{session_clicks.min()}')\n",
    "print(f'max number of clicks per session:{session_clicks.max()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87c03dbedf499cbb4ce724f2e97dc8aa",
     "grade": false,
     "grade_id": "cell-f6996aa4808d7747",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For our new click data, we need a new dataloader.\n",
    "`QueryGroupedOnlineLTRData` provides what you will need for training an LTR model from the clicks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da61ee98e6094a5bf41deff34136be4e",
     "grade": false,
     "grade_id": "cell-036f66cea8bae0bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class QueryGroupedOnlineLTRData(Dataset):\n",
    "    def __init__(self, data, split, clicks, topk):\n",
    "        self.split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert self.split is not None, \"Invalid split!\"\n",
    "        self.clicks = clicks\n",
    "        self.topk = topk\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.split.num_queries()\n",
    "\n",
    "    def __getitem__(self, q_i):\n",
    "        s_i = self.split.doclist_ranges[q_i]\n",
    "        e_i = self.split.doclist_ranges[q_i + 1]\n",
    "        feature_ = self.split.feature_matrix[s_i:e_i,:]\n",
    "        ranking = self.split.ranks[s_i:e_i]\n",
    "        inverse_ranking = np.argsort(ranking)\n",
    "#       We re-order the rows of feature matrix so that in coincides with the ranking of documents.\n",
    "#       We also cut the documents after rank topk.\n",
    "        feature = torch.FloatTensor(feature_[inverse_ranking[:self.topk],:])\n",
    "        clicks = self.clicks[q_i]\n",
    "        return q_i, feature, clicks\n",
    "    \n",
    "def qgo_collate_fn(batch):\n",
    "    \n",
    "    qids = []\n",
    "    features = []\n",
    "    clicks = []\n",
    "    \n",
    "    for (q, f, l) in batch:\n",
    "        qids.append(q)\n",
    "        features.append(f)\n",
    "        clicks.append(l)\n",
    "    return qids, features, clicks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33284e30370a8dfdd116267499660b27",
     "grade": false,
     "grade_id": "cell-3ff364596e375e19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Counterfactual LTR (90 points) <a class=\"anchor\" id=\"cLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Now we want to use the simulated clicks and train a counterfactual LTR algorithm.\n",
    "We will use a simple loss function:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{|Q|} \\sum_{q \\in Q} \\sum_{d \\in D_q} r(d, q) \\cdot \\lambda(\\bar{rank}(d, D_q))\n",
    "$$\n",
    "where $Q$ is the set of queries, $D_q$ is the list of documents shown to the user for query $q$, $r(d, q)$ is the relevance of document $d$ to query $q$ and $rank(d, D_q)$ is the rank of $d$ in $D_q$.\n",
    "\n",
    "In this assignment, we use sigmoid-like bound for $\\lambda(\\bar{rank}(d, D_q))$:\n",
    "\n",
    "$$\n",
    "\\lambda(\\bar{rank}(d, D_q)) = \\sum_{d' \\in D_q} log\\left(\n",
    "1+exp\\left(-2\\cdot(f(d)-f(d')\\right)\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a52232af5a6b442146258942ced99353",
     "grade": false,
     "grade_id": "cell-5d364d940f11536e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.1. Biased LTR (20 points)\n",
    "\n",
    "Remember that we do not have the explicit relevance in the online settings.\n",
    "Instead, we have to learn from the clicks.\n",
    "Clicks, implicitly indicate relevance.\n",
    "A naive approach for learning from clicks would be to replace relevance with clicks and train a model with the loss function:\n",
    "\n",
    "$$\n",
    "L_{biased}=\\frac{1}{|Q|} \\sum_{q \\in Q} \\sum_{d \\in D_q} c(d, q) \\cdot \\lambda(\\bar{rank}(d, D_q))\n",
    "$$\n",
    "where $c(d, q)$ indicates the number of times $d$ was clicked for query $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "defc1880152e68dd36c21992263f8d56",
     "grade": false,
     "grade_id": "cell-6b45762861989c96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def online_loss_biased(scores, clicks):\n",
    "    \n",
    "    from collections import Counter\n",
    "    click_counts = Counter(clicks)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    c_lmd = 0\n",
    "    \n",
    "    scores_list = scores.tolist()\n",
    "    for i,x1  in enumerate(scores_list):\n",
    "        lmd = 0\n",
    "        for i2,x2 in enumerate(scores_list):\n",
    "             lmd += (np.log(1 + np.exp(-2*(x1[0] - x2[0]))))\n",
    "        c_lmd += click_counts[i] * lmd\n",
    "        \n",
    "    return c_lmd\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd1acc24a7415fcae69fa183a51afb28",
     "grade": true,
     "grade_id": "cell-d09e7eb339052376",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = torch.FloatTensor([1.2, -2, 1.1, 2.4, 0, 1.2])[:,None]\n",
    "clicks = [0, 0, 1, 3, 1, 4, 0, 5, 0, 1, 2]\n",
    "print(f'biased loss: {online_loss_biased(scores, clicks)}')\n",
    "print(f'expected: -6.205410003662109')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48efe214b4ffd699be8ebbbf0016b467",
     "grade": false,
     "grade_id": "cell-318d7a3b824c5f99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.2. Unbiased LTR (10 points)\n",
    "Now modify `online_loss_biased` to debias the clicks, using IPS method:\n",
    "\n",
    "$$\n",
    "L_{unbiased}=\\frac{1}{|Q|} \\sum_{q \\in Q} \\sum_{d \\in D_q} \\frac{c(d, q)}{P\\left(o(d,q)=1\\right)} \\cdot \\lambda(\\bar{rank}(d, D_q))\n",
    "$$\n",
    "where $P\\left(o(d,q)=1\\right)$ indicates the probability that $d$ was observed by the user for query $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6e0238d2d1cabe1a3d21b0cc44a54d61",
     "grade": false,
     "grade_id": "cell-9362e909abcb1d91",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def online_loss_unbiased(scores, clicks):\n",
    "    \"\"\"\n",
    "    Compute and return the unbiased online loss *for a single query*. To compute this, use L_{unbiased} formula above.\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    clicks: list of clicked ranks. \n",
    "    \n",
    "    Note 1: the scores are aligned with the click positions, i.e. scores[0] corresponds to the 0 rand, etc.\n",
    "    Note 2: the weights are provided.\n",
    "    \n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    click_counts = Counter(clicks)\n",
    "    weights = torch.Tensor((1./np.arange(1,scores.shape[0]+1))[:,None])\n",
    "    weights_list = weights.tolist()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    c_lmd = 0\n",
    "    \n",
    "    scores_list = scores.tolist()\n",
    "    for i,x1  in enumerate(scores_list):\n",
    "        lmd = 0\n",
    "        for i2,x2 in enumerate(scores_list):\n",
    "             lmd += (np.log(1 + np.exp(-2*(x1[0] - x2[0]))))\n",
    "        c_lmd += click_counts[i] * lmd / weights_list[i][0]\n",
    "                \n",
    "    return c_lmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2bcac0a6c4afe70fca335c8d648122f",
     "grade": true,
     "grade_id": "cell-30b468279fbb6de5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = torch.FloatTensor([1.2, -2, 1.1, 2.4, 0, 1.2])[:,None]\n",
    "clicks = [0, 0, 1, 3, 1, 4, 0, 5, 0, 1, 2]\n",
    "print(f'biased loss: {online_loss_unbiased(scores, clicks)}')\n",
    "print(f'expected: -17.11062240600586')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4f49c8953f6080ec340ebe076f9caf6",
     "grade": false,
     "grade_id": "cell-e385419532bb6535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Implementation (60 points):**\n",
    "In the next cell, write a wrapper that uses your loss functions and simulated clicks to train an online LTR method (no evaluation on validation set is required):\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 10 points\n",
    " - Loss calculation: 10 points\n",
    " - Training (e.g optimizer, zero_grad, backward): 10 points\n",
    " - Performance as expected for biased loss: 15 points\n",
    " - Performance as expected for unbiased loss: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1113e39b6c82d64e959436f049df22ac",
     "grade": false,
     "grade_id": "cell-155a60a13f0dabb3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (60 points)\n",
    "def train_online(net, train_clicks, loss_fn, params):\n",
    "    \"\"\"\n",
    "    Use QueryGroupedOnlineLTRData to load the data train split and clicks.\n",
    "    Use the appropriate loss_fn (biased/unbiased).\n",
    "    No need to use validation set.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from collections import Counter\n",
    "  \n",
    "    train_dl = DataLoader(QueryGroupedOnlineLTRData(data, \"train\", train_clicks, params.topk), batch_size=1, shuffle=True, collate_fn=qgo_collate_fn) \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = params.lr)\n",
    "    \n",
    "    count_new_click = []\n",
    "\n",
    "   \n",
    "    for epoch in range(params.epochs):\n",
    "        for (_, features,train_clicks) in train_dl:\n",
    "            count_new_click.clear()\n",
    "            count_lst = Counter(train_clicks[0])\n",
    "            for i in range(0,features[0].shape[0]):\n",
    "                if i in count_lst.keys():\n",
    "                    count_new_click.append(count_lst[i])\n",
    "                else:\n",
    "                    count_new_click.append(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(features[0])\n",
    "            myloss = loss_fn(outputs,count_new_click)\n",
    "            #print (\"myloss\", type(myloss) , myloss)\n",
    "            myloss = torch.tensor(myloss)\n",
    "            myloss.requires_grad=True\n",
    "            #print(\"myloss\", type(myloss) , myloss)\n",
    "            myloss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "688a802dd604c3f002a9c5b26822b42d",
     "grade": false,
     "grade_id": "cell-2899cebacae54c00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = Namespace(epochs=3, lr=1e-3, topk=20)\n",
    "train_clicks = simulate_clicks(data,'train', click_count = 50000)\n",
    "biased_net = NeuralModule(1)\n",
    "train_online(biased_net, train_clicks, online_loss_biased, params)\n",
    "unbiased_net = NeuralModule(1)\n",
    "train_online(unbiased_net, train_clicks, online_loss_unbiased, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ff8105a6912656bef1bf2642a977fee",
     "grade": false,
     "grade_id": "cell-0511eb2be587f032",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(biased_net, 'test', print_results=True)\n",
    "evaluate_model(unbiased_net, 'test', print_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a5138ee2bdc3b5682d5ed8b7bcc1202",
     "grade": false,
     "grade_id": "cell-c497608f3d46e769",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "train_clicks = simulate_clicks(data,'train', click_count = 50000)\n",
    "params = Namespace(epochs=3, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=1,\n",
    "                    topk=20,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "biased_model = NeuralModule(1)\n",
    "\n",
    "create_results(biased_model, \n",
    "               train_online, \n",
    "               biased_model,\n",
    "               \"./biased_model.json\",\n",
    "               train_clicks,\n",
    "               online_loss_biased,\n",
    "               params)\n",
    "# persist model\n",
    "torch.save(biased_model.state_dict(), \"./biased_wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9b104b0cc6204f1f3c6ac9bbf88171f",
     "grade": false,
     "grade_id": "cell-9df7065c5b887449",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "\n",
    "params = Namespace(epochs=3, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=1,\n",
    "                    topk=20,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "unbiased_model = NeuralModule(1)\n",
    "\n",
    "create_results(unbiased_model, \n",
    "               train_online, \n",
    "               unbiased_model,\n",
    "               \"./unbiased_model.json\",\n",
    "               train_clicks,\n",
    "               online_loss_unbiased,\n",
    "               params)\n",
    "# persist model\n",
    "torch.save(biased_model.state_dict(), \"./unbiased_wt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ede3bc5a61edfb93777647c3ae0b1a",
     "grade": true,
     "grade_id": "cell-5a98cb1d4dccab54",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34fad8b3eebaaca7a38cf37fb55433e3",
     "grade": true,
     "grade_id": "cell-6eaff35ed2f350ea",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8cc768ddc4dd12732fc4c1b42cf015d",
     "grade": true,
     "grade_id": "cell-6a1e7d45a6b24c7c",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca775c32f1df7ef6d70c60b3e365adf4",
     "grade": false,
     "grade_id": "cell-08980b08558f70e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 2.3: Comparison (20 points)\n",
    "\n",
    "Now we can compare two loss functions: biased and unbiased.\n",
    "We want to see how they can improve by increasing the number of training clicks.\n",
    "Train both biased and unbaised models on $[2000, 10000, 50000]$ number of clicks and compare the results.\n",
    "\n",
    "Plot the results in a **single** figure with x-axis showing the number of trainin clicks and y-axis showing the nDCG@10.\n",
    "Discuss your observations.\n",
    "\n",
    "\n",
    "**Rubric:**\n",
    "- Two curves are plotted in the figure: 10 points\n",
    "- Clear titles, x label, y labels and legends (if applicable): 5 points\n",
    "- Explain what you observe: 5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76693360dd6493e5811d1908eacf8227",
     "grade": true,
     "grade_id": "cell-f51a0be2734c9352",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "biased= [1,2,3]\n",
    "unbiased = [4,5,6]\n",
    "\n",
    "def plot (biased, unbiased):\n",
    "    clicks = [2000,10000,50000]\n",
    "    plt.plot (clicks, biased, 'r-', label='biased')\n",
    "    plt.plot (clicks, unbiased, 'b-', label='unbiased')\n",
    "    plt.xlabel(\"clicks\")\n",
    "    plt.ylabel(\"nDCG@10\")\n",
    "    plt.title (\"biased and unbiased ranking model\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"biased_unbiased_comparison\", dpi=300)\n",
    "    plt.show()\n",
    "plot (biased, unbiased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce47393cb34036b50fde6780cdb9f772",
     "grade": false,
     "grade_id": "cell-6bd0ff7cecc1b060",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 3: Online Evaluation (75 points) <a class=\"anchor\" id=\"on_eval\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Sometimes, in online search engines, we want to compare two or more different ranking functions based on the user interactions.\n",
    "This comparison is done via online evaluation.\n",
    "\n",
    "In this section we implement one of the online evaluation methods: probabilistic multileaving.\n",
    "\n",
    "We compare three rankers:\n",
    " - Production ranker: the ranks that are provided to you for doing the click simulation.\n",
    " - Biased method from previous section.\n",
    " - Unbiased method from previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "590bdae1272cdd45e304d0498de730bc",
     "grade": false,
     "grade_id": "cell-6ca9cf6f08e6b7f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before proceeding, we need some auxiliary functions:\n",
    "\n",
    " - `invert_ranking` gets the ranking and gives the inverted rankings. This means that for an input with `rank[d]=r`, the output would be `i_rank[r]=d`.\n",
    "\n",
    "\n",
    " - `get_predictions` gives a dictionary of predictions, i.e. `predictions[qid]` is the list of scores produced by the given model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98f77357b3d3088c738c97a90bd05061",
     "grade": false,
     "grade_id": "cell-3ae14fda3426a247",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_predictions(net, test_clicks, topk):\n",
    "    test_dl = DataLoader(QueryGroupedOnlineLTRData(data, 'test', test_clicks, topk), \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          collate_fn=qg_collate_fn)\n",
    "    predictions = {}\n",
    "    for qids, x, _ in test_dl:\n",
    "        predictions[qids[0]] = net(x[0]).detach().numpy()[:,0]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def invert_rankings(rankings):\n",
    "    '''\n",
    "    Invert indices in a matrix of rankings, ranking per row.\n",
    "    '''\n",
    "    inverted = np.zeros(rankings.shape)\n",
    "    \n",
    "    inverted[np.arange(rankings.shape[0])[:,None],rankings] = np.arange(rankings.shape[1])[None,:]\n",
    "    return inverted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "715b799e36aa757ed9db3dd60c9b724e",
     "grade": false,
     "grade_id": "cell-7bbf6c13e9eaea91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The next cell helps you build the ranking matrix for each query in the test set.\n",
    "The first row is the production ranker, the second row is the biased ranker and the third row is the unbiased ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f2a6904a62575980830a95ed5ee238d",
     "grade": false,
     "grade_id": "cell-3bdf5ac68c468d35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# we only need the ranking of the production ranker for the test set. No clicks have to be simulated here.\n",
    "test_clicks = simulate_clicks(data, 'test', click_count = 0)\n",
    "biased_predictions = get_predictions(biased_net, test_clicks, 20)\n",
    "unbiased_prediction = get_predictions(unbiased_net, test_clicks, 20)\n",
    "\n",
    "def get_ranking_matrix(qid, topk):\n",
    "    ranking_matrix = np.empty([3, min(data.test.query_size(qid), topk)], dtype=np.int32)\n",
    "    ranking = data.test.ranks[data.test.doclist_ranges[qid]:data.test.doclist_ranges[qid+1]]\n",
    "    ranking_matrix[0,:] = np.arange(ranking_matrix.shape[1])\n",
    "    ranking_matrix[1,:] = np.argsort(-biased_predictions[qid])\n",
    "    ranking_matrix[2,:] = np.argsort(-unbiased_prediction[qid])\n",
    "    return ranking_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5cc0c513947abfc38bd19f364c24992",
     "grade": false,
     "grade_id": "cell-77d4e88031772611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(get_ranking_matrix(0, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3a1de51e80c9251462e68fb16b4fb0f",
     "grade": false,
     "grade_id": "cell-ff251675418a5a1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.1 Multileaving (10 points)\n",
    "\n",
    "Given the rankings of multiple rankers, we want to decide how to fill the results list and show it to the users.\n",
    "Implement the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1f9c735274c6a469bd089afd078bcf9",
     "grade": false,
     "grade_id": "cell-fcf5351ca3d36caa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def make_multileaving(inverted_rankings, topk):\n",
    "    '''\n",
    "    ARGS: (all np.array of docids)\n",
    "    - inverted_rankings: matrix (rankers x documents) where [x,y] corresponds to the rank of doc y in ranker x\n",
    "    RETURNS\n",
    "    - ranking of indices corresponding to inverted_rankings\n",
    "    '''\n",
    "     n_rankers = inverted_rankings.shape[0]\n",
    "    n = inverted_rankings.shape[1]\n",
    "    k = min(n, topk)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    tau = 3\n",
    "    \n",
    "    #calculation of initial P(A), P(B)\n",
    "    P_all = []\n",
    "    sum_all = []\n",
    "    result = []\n",
    "    \n",
    "    for rank in range(n_rankers):\n",
    "        sum_per_rank = 0\n",
    "        for i in range(k):\n",
    "            sum_per_rank += 1/(inverted_rankings[rank][i]**tau)\n",
    "        sum_all.append(sum_per_rank)\n",
    "        \n",
    "    for rank in range(n_rankers):\n",
    "        all_doc = []\n",
    "        for i in range(k):\n",
    "            d_i = (1/(inverted_rankings[rank][i]**tau))/sum_all[rank]\n",
    "            all_doc.append(d_i)\n",
    "        P_all.append(all_doc)\n",
    "    \n",
    "    #placing the docs\n",
    "    choices = np.arange(n_rankers)\n",
    "    for doc_ranked in range(k): \n",
    "        choice = np.random.choice(choices)\n",
    "        best_doc = max(P_all[choice])\n",
    "        index = P_all[choice].index(best_doc)\n",
    "        result.append(index)\n",
    "        \n",
    "        new_array\n",
    "        for i in inverted_rankings:\n",
    "            np.delete(i, [index]) #deleting part does not work properly, since I cannot just delete elements in a loop here\n",
    "            \n",
    "        #recalculating P_all\n",
    "        P_all = []\n",
    "        sum_all = []\n",
    "        \n",
    "        for rank in range(n_rankers):\n",
    "            sum_per_rank = 0\n",
    "            for i in range(k):\n",
    "                sum_per_rank += 1/(inverted_rankings[rank][i]**tau)\n",
    "            sum_all.append(sum_per_rank)\n",
    "            \n",
    "        for rank in range(n_rankers):\n",
    "            all_doc = []\n",
    "            for i in range(k):\n",
    "                d_i = (1/(inverted_rankings[rank][i]**tau))/sum_all[rank]\n",
    "                all_doc.append(d_i)\n",
    "            P_all.append(all_doc)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_multileaving(invert_rankings(get_ranking_matrix(0, 20)), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51819e8bd101b87700217e0ea4983c40",
     "grade": true,
     "grade_id": "cell-a7f83f82f0a41b58",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca3e1ec97c81db9c4a9e3d7ec81919c",
     "grade": false,
     "grade_id": "cell-024ab191ba5fc1c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.2: Probability of rankers (15 points)\n",
    "\n",
    "We have shown the multileaved list to the user and they clicked on some documents.\n",
    "In this function we want to calculate the probability that the clicked documents belong to a specific ranker.\n",
    "\n",
    "Note that the results of rankers are not unique and each document in the results list may belong to different rankers.\n",
    "So we need to assign a *probability* to each click belonging to each ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6150d058189bef659154cdb1e700d18",
     "grade": false,
     "grade_id": "cell-06a948902936dd40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (15 points)\n",
    "def probability_of_list(result_list, inverted_rankings, clicked_docs):\n",
    "    '''\n",
    "    ARGS: (all np.array of docids)\n",
    "    - result_list: the multileaved list\n",
    "    - inverted_rankings: matrix (rankers x documents) where [x,y] corresponds to the rank of doc y in ranker x\n",
    "    - clicked_docs: boolean array of result_list length indicating clicks\n",
    "    RETURNS\n",
    "    -sigmas: matrix (rankers x clicked_docs) with probabilty ranker added clicked doc\n",
    "    '''\n",
    "    n_docs = inverted_rankings.shape[1]\n",
    "    n_rankers = inverted_rankings.shape[0]\n",
    "\n",
    "    click_doc_ind = result_list[clicked_docs]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216c6987df2ad52d86dec05976823892",
     "grade": false,
     "grade_id": "cell-cb41d992a09ed02e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ranking_matrix = get_ranking_matrix(0, 20)\n",
    "result_list = make_multileaving(invert_rankings(ranking_matrix), 20)\n",
    "clicks = [0,4,9,10]\n",
    "probabilities = probability_of_list(result_list, invert_rankings(ranking_matrix), clicks)\n",
    "print(f'ranking matrix:\\n {ranking_matrix}')\n",
    "print(f'results list (shown to user):\\n {result_list}')\n",
    "print(f'clicked documents: {clicks}')\n",
    "print(f'probabilities:\\n {probabilities}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b4921314264ee89e72d616cf8ee8f33",
     "grade": true,
     "grade_id": "cell-41eadc2f4faaf437",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cdd78a87da0a5d3a05207c5e926eef4",
     "grade": false,
     "grade_id": "cell-95c4e4ae05ce7198",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.3: Preference matrix (10 points)\n",
    "\n",
    "Given the probabilities of each ranker being clicked, we want to calculate a preference matrix that for each pair of rankers tells us which one is preferred by the clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec8f2075b4b94720bdf96898d4254589",
     "grade": false,
     "grade_id": "cell-931bf5baba8e9eac",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "def preferences_of_list(probs):\n",
    "    '''\n",
    "    ARGS:\n",
    "    -probs: clicked docs x rankers matrix with probabilities ranker added clicked doc  (use probability_of_list)\n",
    "    -n_samples: number of samples to base preference matrix on\n",
    "    RETURNS:\n",
    "    - preference matrix: matrix (rankers x rankers) in this matrix [x,y] > 0 means x won over y and [x,y] < 0 means x lost from y\n",
    "      the value is analogous to the (average) degree of preference\n",
    "    '''\n",
    "    n_samples = 10\n",
    "    n_clicks = probs.shape[0]\n",
    "    n_rankers = probs.shape[1]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def infer_preferences(inverted_rankings, result_list, clicked_docs):\n",
    "    n_rankers = inverted_rankings.shape[0]\n",
    "    if np.any(clicked_docs):\n",
    "        return preferences_of_list(probability_of_list(result_list,\n",
    "                                        inverted_rankings,\n",
    "                                        clicked_docs))\n",
    "    else:\n",
    "        return np.zeros((n_rankers, n_rankers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2342811f833dbdbc29a5f7f5725b576",
     "grade": false,
     "grade_id": "cell-9598544608fbcde8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(preferences_of_list(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e79d7a1992af19a5fcd6711ece693800",
     "grade": true,
     "grade_id": "cell-601defbd1e14a840",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0360fb3c0a060e97efd327ec0cc79468",
     "grade": false,
     "grade_id": "cell-766b24e1ede46a8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can simulate the multileaving to see how our target rankers are evaluated by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d071088604059a588482912e2a0dfd2",
     "grade": false,
     "grade_id": "cell-45aaaa7c7a055fed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simulate_multileaving(n_impressions, topk):\n",
    "    n_samples = 10\n",
    "    impressions = 0\n",
    "    pref_update = 0\n",
    "    total_pref = np.zeros((3, 3))\n",
    "    for step_i in range(n_impressions):\n",
    "        qid = np.random.randint(0, data.test.doclist_ranges.shape[0] - 1)\n",
    "\n",
    "        start_i = data.test.doclist_ranges[qid]\n",
    "        end_i = data.test.doclist_ranges[qid + 1]\n",
    "        n_query_docs = end_i - start_i\n",
    "        query_labels = data.test.label_vector[start_i:end_i]\n",
    "\n",
    "        inverted_rankings = invert_rankings(get_ranking_matrix(qid, topk))\n",
    "        multileaving = make_multileaving(inverted_rankings, topk)\n",
    "\n",
    "        cur_clicks = generate_clicks(multileaving, query_labels)\n",
    "\n",
    "        if np.any(cur_clicks):\n",
    "            pref = infer_preferences(inverted_rankings, multileaving, cur_clicks)\n",
    "            total_pref += pref\n",
    "            pref_update += 1\n",
    "    return total_pref / pref_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95b61f7faa7cb7febe3d00fb66d0cda3",
     "grade": false,
     "grade_id": "cell-eae8a938c37a37f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "simulate_multileaving(10000, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29bb96f857fa7ae1dc2d816a1b888d7b",
     "grade": false,
     "grade_id": "cell-04ee05c85dd0304d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Section 3.4: Analysis (40 points)\n",
    "\n",
    "Analyze the behavior of your multileaving implementation by testing it with two different `topk` values: $[5,20]$ and with different number of evaluation clicks: $[2000, 10000, 50000]$.\n",
    "\n",
    "Put the preference of the unbiased method over the biased method (i.e. `pref[2,1]` in the matrix output of `simulate_multileaving` function) for these experiments in a table.\n",
    "\n",
    "**Rubric:**\n",
    "- Six experiments: 12 points\n",
    "- Analysis of the observations: 28 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd41d14078b1ad160c02cf690c45e55f",
     "grade": true,
     "grade_id": "cell-d88f4eb3035e6fbb",
     "locked": false,
     "points": 40,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
